{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HBB COMPLETE VALIDATION PIPELINE (FULLY OFFLINE)\n",
      "================================================================================\n",
      "\n",
      "Gene: HBB\n",
      "Output: .cache/HBB_protein_level_validated.csv\n",
      "\n",
      "üåê Network requirements: NONE (all validation is offline)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "LOADING LOCAL REFERENCE DATA\n",
      "================================================================================\n",
      "‚úÖ Found: /jet/home/barazand/NEWOCEAN/ref_data/ensembl/Homo_sapiens.GRCh38.112.gtf.gz\n",
      "‚úÖ Found: /jet/home/barazand/NEWOCEAN/ref_data/ensembl/Homo_sapiens.GRCh38.pep.all.fa.gz\n",
      "‚úÖ Found: /jet/home/barazand/NEWOCEAN/ref_data/mane/MANE.GRCh38.v1.3.summary.txt.gz\n",
      "‚úÖ Found: /jet/home/barazand/NEWOCEAN/ref_data/uniprot/uniprot_sprot.fasta.gz\n",
      "‚úÖ Found: /jet/home/barazand/NEWOCEAN/ref_data/uniprot/HUMAN_9606_idmapping.dat.gz\n",
      "\n",
      "üìö Loading Ensembl reference data (GTF + peptides)...\n",
      "   Parsing GTF: Homo_sapiens.GRCh38.112.gtf.gz\n",
      "      Found 111983 ENST‚ÜíENSP mappings\n",
      "   Parsing peptide FASTA: Homo_sapiens.GRCh38.pep.all.fa.gz\n",
      "      Found 246990 ENSP‚Üísequence mappings\n",
      "   ‚úÖ Built 111983 ENST‚Üísequence mappings\n",
      "\n",
      "üìö Loading MANE summary...\n",
      "   Parsing MANE summary: MANE.GRCh38.v1.3.summary.txt.gz\n",
      "      Found 19288 MANE Select transcripts\n",
      "\n",
      "üìö Loading UniProt data...\n",
      "   Parsing UniProt ID mapping: HUMAN_9606_idmapping.dat.gz\n",
      "      Found 26376 gene‚ÜíUniProt mappings\n",
      "   Parsing UniProt FASTA: uniprot_sprot.fasta.gz\n",
      "      Found 573661 UniProt‚Üísequence mappings\n",
      "\n",
      "‚úÖ All reference data loaded successfully\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 1: FETCHING GNOMAD DATA FOR HBB\n",
      "================================================================================\n",
      "\n",
      "üìä Getting gene information...\n",
      "   Gene: HBB\n",
      "   Chromosome: 11\n",
      "   Position: 5,225,464 - 5,227,071\n",
      "   HGNC ID: HGNC:4827\n",
      "\n",
      "üîç Querying gnomAD v4...\n",
      "   ‚úÖ Fetched 8946 variant-transcript pairs\n",
      "   ‚úÖ Saved to: .cache/gnomAD_HBB.json\n",
      "\n",
      "================================================================================\n",
      "STEP 2: PROCESSING PROTEIN-LEVEL INFORMATION\n",
      "================================================================================\n",
      "\n",
      "üîç Filtering for missense variants...\n",
      "   Total missense: 1057\n",
      "\n",
      "üß¨ Parsing HGVSp notation...\n",
      "   Parsed: 1057/1057\n",
      "\n",
      "üìå Filtering for canonical transcripts...\n",
      "   Canonical: 432\n",
      "\n",
      "üî¨ Attaching protein sequences from local Ensembl cache...\n",
      "üìä Sequences attached: 216/432\n",
      "\n",
      "‚úÖ Verifying sequences...\n",
      "   Verified: 216/432\n",
      "\n",
      "üîÑ Removing genomic duplicates...\n",
      "   Before: 432\n",
      "   After: 170\n",
      "\n",
      "================================================================================\n",
      "STEP 3: INTERNAL VALIDATION\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  Required columns...\n",
      "   ‚úÖ All present\n",
      "\n",
      "2Ô∏è‚É£  Null values...\n",
      "   ‚úÖ ref_aa: no nulls\n",
      "   ‚úÖ protein_pos: no nulls\n",
      "   ‚úÖ alt_aa: no nulls\n",
      "   ‚úÖ protein_seq: no nulls\n",
      "\n",
      "3Ô∏è‚É£  Amino acid validity...\n",
      "   ‚úÖ All ref_aa valid\n",
      "   ‚úÖ All alt_aa valid\n",
      "\n",
      "4Ô∏è‚É£  Sequence verification...\n",
      "   Verified: 170/170 (with sequences)\n",
      "\n",
      "5Ô∏è‚É£  Duplicates...\n",
      "   ‚úÖ No genomic duplicates\n",
      "   ‚ö†Ô∏è  Protein: 8 (OK - degeneracy)\n",
      "\n",
      "6Ô∏è‚É£  Allele frequencies...\n",
      "   ‚úÖ All AF in [0,1]\n",
      "\n",
      "================================================================================\n",
      "STEP 4: EXTERNAL VALIDATION (ALL OFFLINE)\n",
      "================================================================================\n",
      "\n",
      "üìÇ Your data:\n",
      "   Transcript: ENST00000335295\n",
      "   Sequence: 147 aa\n",
      "   Variants: 170\n",
      "\n",
      "üî¨ Querying UniProt (LOCAL)...\n",
      "   ‚úÖ UniProt: P68871\n",
      "   Length: 147 aa\n",
      "   ‚úÖ Sequence MATCHES UniProt (100%)\n",
      "\n",
      "üìã Checking MANE Select (LOCAL)...\n",
      "   ‚úÖ MANE Select ENST for HBB: ENST00000335295\n",
      "   ‚úÖ Using MANE Select transcript\n",
      "   ‚úÖ Corresponding RefSeq (NM): NM_000518.5\n",
      "   ‚úÖ MANE Select sequence: 147 aa\n",
      "   ‚úÖ Your sequence MATCHES MANE Select\n",
      "\n",
      "üîç Cross-database sequence comparison:\n",
      "   ‚úÖ ALL SEQUENCES IDENTICAL across 3 databases\n",
      "\n",
      "üîç Validating gnomAD annotations...\n",
      "   ‚úÖ Amino acids consistent (170 variants)\n",
      "   ‚úÖ All variants are missense\n",
      "\n",
      "================================================================================\n",
      "STEP 5: ADDING STRUCTURE IDS\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Added structure IDs:\n",
      "   UniProt: P68871\n",
      "   AlphaFold: AF-P68871-F1\n",
      "\n",
      "================================================================================\n",
      "STEP 6: GENERATING VALIDATION REPORT\n",
      "================================================================================\n",
      "‚úÖ Report saved: .cache/HBB_validation_report.txt\n",
      "\n",
      "================================================================================\n",
      "SAVING FINAL DATA\n",
      "================================================================================\n",
      "‚úÖ CSV: .cache/HBB_protein_level_validated.csv\n",
      "‚úÖ JSON: .cache/HBB_protein_level_validated.json\n",
      "\n",
      "================================================================================\n",
      "PIPELINE COMPLETE (FULLY OFFLINE)\n",
      "================================================================================\n",
      "\n",
      "üéâ ‚úÖ SUCCESS - DATA IS VALIDATED!\n",
      "\n",
      "   Total variants: 170\n",
      "   UniProt ID: P68871 (from local cache)\n",
      "   ‚úÖ Sequences validated across 3 databases (all local)\n",
      "\n",
      "   ‚ö†Ô∏è  1 warnings (non-critical)\n",
      "\n",
      "   ‚úÖ SAFE TO PROCEED WITH ANALYSIS\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìä Sample output:\n",
      "CHROM     POS mutation           AF uniprot_id alphafold_id\n",
      "   11 5225611    H144R 6.840670e-07     P68871 AF-P68871-F1\n",
      "   11 5225612    H144Y 6.572810e-06     P68871 AF-P68871-F1\n",
      "   11 5225615    A143T 1.368160e-06     P68871 AF-P68871-F1\n",
      "   11 5225626    A139V 6.840870e-07     P68871 AF-P68871-F1\n",
      "   11 5225630    V138M 6.840720e-07     P68871 AF-P68871-F1\n",
      "   11 5225632    G137D 6.571340e-06     P68871 AF-P68871-F1\n",
      "   11 5225635    A136V 4.788810e-06     P68871 AF-P68871-F1\n",
      "   11 5225638    V135A 4.600660e-05     P68871 AF-P68871-F1\n",
      "   11 5225638    V135G 6.840850e-07     P68871 AF-P68871-F1\n",
      "   11 5225638    V135E 6.572370e-06     P68871 AF-P68871-F1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete HBB Variant Processing and Validation Pipeline (Fully Offline)\n",
    "\n",
    "This script:\n",
    "1. Fetches gnomAD data for HBB\n",
    "2. Processes and parses protein-level information using LOCAL reference files\n",
    "3. Validates internally (consistency checks)\n",
    "4. Validates externally (local UniProt, local Ensembl, local MANE)\n",
    "5. Adds UniProt and AlphaFold IDs\n",
    "6. Saves validated data\n",
    "\n",
    "ALL validation is done using local reference files - NO online queries.\n",
    "\n",
    "Usage:\n",
    "    python process_hbb_fully_offline.py\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "from variant_utils.get_gene_info import get_gene_info\n",
    "from variant_utils.gnomad_utils import queryGnomAD\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Set Java environment\n",
    "os.environ['JAVA_HOME'] = '/jet/home/barazand/NEWOCEAN/java/jdk-21.0.9'\n",
    "os.environ['PATH'] = f\"/jet/home/barazand/NEWOCEAN/java/jdk-21.0.9/bin:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "# Paths\n",
    "CACHE_DIR = Path(\".cache\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)\n",
    "EXTERNAL_TOOLS_CONFIG = \"external_tools.json\"\n",
    "\n",
    "# Local reference data paths\n",
    "REFERENCE_DIR = Path(\"/jet/home/barazand/NEWOCEAN/ref_data\")  # adjust as needed\n",
    "ENSEMBL_GTF = REFERENCE_DIR / \"ensembl\" / \"Homo_sapiens.GRCh38.112.gtf.gz\"\n",
    "ENSEMBL_PEP = REFERENCE_DIR / \"ensembl\" / \"Homo_sapiens.GRCh38.pep.all.fa.gz\"\n",
    "MANE_SUMMARY = REFERENCE_DIR / \"mane\" / \"MANE.GRCh38.v1.3.summary.txt.gz\"\n",
    "UNIPROT_FASTA = REFERENCE_DIR / \"uniprot\" / \"uniprot_sprot.fasta.gz\"\n",
    "UNIPROT_IDMAPPING = REFERENCE_DIR / \"uniprot\" / \"HUMAN_9606_idmapping.dat.gz\"\n",
    "\n",
    "# Gene to process\n",
    "GENE_SYMBOL = \"HBB\"\n",
    "\n",
    "# Output files\n",
    "OUTPUT_CSV = CACHE_DIR / f\"{GENE_SYMBOL}_protein_level_validated.csv\"\n",
    "OUTPUT_JSON = CACHE_DIR / f\"{GENE_SYMBOL}_protein_level_validated.json\"\n",
    "VALIDATION_REPORT = CACHE_DIR / f\"{GENE_SYMBOL}_validation_report.txt\"\n",
    "\n",
    "# Global caches (loaded once at startup)\n",
    "ENST_TO_SEQ = None\n",
    "GENE_TO_MANE_ENST = None\n",
    "ENST_TO_NM = None\n",
    "GENE_TO_UNIPROT = None\n",
    "UNIPROT_TO_SEQ = None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOCAL REFERENCE LOADERS - ENSEMBL\n",
    "# ============================================================================\n",
    "\n",
    "def load_enst_to_ensp(gtf_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse Ensembl GTF to build mapping transcript_id (ENST) -> protein_id (ENSP).\n",
    "    Maps both versioned and versionless ENST/ENSP.\n",
    "    \"\"\"\n",
    "    print(f\"   Parsing GTF: {gtf_path.name}\")\n",
    "    enst_to_ensp = {}\n",
    "    \n",
    "    with gzip.open(gtf_path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "            feature_type = parts[2]\n",
    "            if feature_type != \"CDS\":\n",
    "                continue\n",
    "            attrs = parts[8]\n",
    "\n",
    "            # Parse attributes\n",
    "            attr_dict = {}\n",
    "            for field in attrs.split(\";\"):\n",
    "                field = field.strip()\n",
    "                if not field or \" \" not in field:\n",
    "                    continue\n",
    "                key, value = field.split(\" \", 1)\n",
    "                attr_dict[key] = value.strip('\"')\n",
    "\n",
    "            tid = attr_dict.get(\"transcript_id\")\n",
    "            pid = attr_dict.get(\"protein_id\")\n",
    "            if not tid or not pid:\n",
    "                continue\n",
    "\n",
    "            # Store versioned and versionless\n",
    "            enst_full = tid\n",
    "            ensp_full = pid\n",
    "            enst_base = tid.split(\".\")[0]\n",
    "            ensp_base = pid.split(\".\")[0]\n",
    "\n",
    "            enst_to_ensp.setdefault(enst_full, ensp_full)\n",
    "            enst_to_ensp.setdefault(enst_base, ensp_full)\n",
    "    \n",
    "    print(f\"      Found {len(enst_to_ensp)} ENST‚ÜíENSP mappings\")\n",
    "    return enst_to_ensp\n",
    "\n",
    "\n",
    "def load_ensp_to_seq(pep_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse Ensembl protein FASTA: ENSP -> AA sequence.\n",
    "    Maps both versioned and versionless ENSP IDs.\n",
    "    \"\"\"\n",
    "    print(f\"   Parsing peptide FASTA: {pep_path.name}\")\n",
    "    ensp_to_seq = {}\n",
    "    current_id = None\n",
    "    seq_chunks = []\n",
    "\n",
    "    with gzip.open(pep_path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # Flush previous\n",
    "                if current_id is not None:\n",
    "                    seq = \"\".join(seq_chunks)\n",
    "                    ensp_to_seq.setdefault(current_id, seq)\n",
    "                    base = current_id.split(\".\")[0]\n",
    "                    ensp_to_seq.setdefault(base, seq)\n",
    "\n",
    "                # Parse new header (first token is ENSP ID)\n",
    "                header = line[1:].strip()\n",
    "                current_id = header.split()[0]\n",
    "                seq_chunks = []\n",
    "            else:\n",
    "                seq_chunks.append(line.strip())\n",
    "\n",
    "        # Flush last\n",
    "        if current_id is not None:\n",
    "            seq = \"\".join(seq_chunks)\n",
    "            ensp_to_seq.setdefault(current_id, seq)\n",
    "            base = current_id.split(\".\")[0]\n",
    "            ensp_to_seq.setdefault(base, seq)\n",
    "\n",
    "    print(f\"      Found {len(ensp_to_seq)} ENSP‚Üísequence mappings\")\n",
    "    return ensp_to_seq\n",
    "\n",
    "\n",
    "def build_enst_to_seq(gtf_path: Path, pep_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Combine GTF and peptide FASTA to build ENST -> protein sequence mapping.\n",
    "    \"\"\"\n",
    "    enst_to_ensp = load_enst_to_ensp(gtf_path)\n",
    "    ensp_to_seq = load_ensp_to_seq(pep_path)\n",
    "\n",
    "    enst_to_seq = {}\n",
    "    for enst, ensp in enst_to_ensp.items():\n",
    "        # Try versioned, then versionless ENSP\n",
    "        seq = ensp_to_seq.get(ensp) or ensp_to_seq.get(ensp.split(\".\")[0])\n",
    "        if seq:\n",
    "            enst_to_seq[enst] = seq\n",
    "            # Also store versionless transcript key\n",
    "            base = enst.split(\".\")[0]\n",
    "            enst_to_seq.setdefault(base, seq)\n",
    "    \n",
    "    print(f\"   ‚úÖ Built {len(enst_to_seq)} ENST‚Üísequence mappings\")\n",
    "    return enst_to_seq\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOCAL REFERENCE LOADERS - MANE\n",
    "# ============================================================================\n",
    "\n",
    "def load_mane_summary(summary_path: Path):\n",
    "    \"\"\"\n",
    "    Load MANE summary table, build:\n",
    "      - gene_symbol -> MANE Select ENST\n",
    "      - ENST -> NM_ RefSeq transcript\n",
    "    \"\"\"\n",
    "    print(f\"   Parsing MANE summary: {summary_path.name}\")\n",
    "    gene_to_mane_enst = {}\n",
    "    enst_to_nm = {}\n",
    "\n",
    "    with gzip.open(summary_path, \"rt\") as f:\n",
    "        header = f.readline().strip().split(\"\\t\")\n",
    "        col_idx = {name: i for i, name in enumerate(header)}\n",
    "\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != len(header):\n",
    "                continue\n",
    "\n",
    "            # Get MANE status\n",
    "            status_col = col_idx.get(\"MANE_status\") or col_idx.get(\"MANE_Status\")\n",
    "            if status_col is None:\n",
    "                continue\n",
    "            status = parts[status_col]\n",
    "            \n",
    "            if status not in (\"MANE Select\", \"MANE_Select\"):\n",
    "                continue\n",
    "\n",
    "            # Get transcript IDs\n",
    "            enst = parts[col_idx.get(\"Ensembl_nuc\", col_idx.get(\"Ensembl_transcript\", -1))]\n",
    "            nm = parts[col_idx.get(\"RefSeq_nuc\", col_idx.get(\"RefSeq_transcript\", -1))]\n",
    "            symbol = parts[col_idx.get(\"symbol\", col_idx.get(\"HGNC_symbol\", -1))]\n",
    "\n",
    "            if not enst or not symbol:\n",
    "                continue\n",
    "\n",
    "            # Store mapping (versionless)\n",
    "            enst_base = enst.split(\".\")[0]\n",
    "            gene_to_mane_enst.setdefault(symbol, enst_base)\n",
    "            if nm:\n",
    "                enst_to_nm.setdefault(enst_base, nm)\n",
    "\n",
    "    print(f\"      Found {len(gene_to_mane_enst)} MANE Select transcripts\")\n",
    "    return gene_to_mane_enst, enst_to_nm\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOCAL REFERENCE LOADERS - UNIPROT\n",
    "# ============================================================================\n",
    "\n",
    "def load_uniprot_id_mapping(idmapping_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load UniProt ID mapping file to build gene_symbol -> UniProt ID mapping.\n",
    "    \n",
    "    File format: UniProtKB-AC <tab> ID_type <tab> ID\n",
    "    Example: P68871\tGene_Name\tHBB\n",
    "    \"\"\"\n",
    "    print(f\"   Parsing UniProt ID mapping: {idmapping_path.name}\")\n",
    "    gene_to_uniprot = {}\n",
    "    \n",
    "    with gzip.open(idmapping_path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) != 3:\n",
    "                continue\n",
    "            \n",
    "            uniprot_id, id_type, value = parts\n",
    "            \n",
    "            # We're interested in Gene_Name mappings\n",
    "            if id_type == \"Gene_Name\":\n",
    "                gene_symbol = value\n",
    "                # Store only if not already present (first entry = primary)\n",
    "                if gene_symbol not in gene_to_uniprot:\n",
    "                    gene_to_uniprot[gene_symbol] = uniprot_id\n",
    "    \n",
    "    print(f\"      Found {len(gene_to_uniprot)} gene‚ÜíUniProt mappings\")\n",
    "    return gene_to_uniprot\n",
    "\n",
    "\n",
    "def load_uniprot_sequences(fasta_path: Path) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load UniProt FASTA to build UniProt ID -> sequence mapping.\n",
    "    \n",
    "    FASTA header format: >sp|P68871|HBB_HUMAN ...\n",
    "    We extract the UniProt accession (P68871)\n",
    "    \"\"\"\n",
    "    print(f\"   Parsing UniProt FASTA: {fasta_path.name}\")\n",
    "    uniprot_to_seq = {}\n",
    "    current_id = None\n",
    "    seq_chunks = []\n",
    "    \n",
    "    with gzip.open(fasta_path, \"rt\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # Flush previous\n",
    "                if current_id is not None:\n",
    "                    seq = \"\".join(seq_chunks)\n",
    "                    uniprot_to_seq[current_id] = seq\n",
    "                \n",
    "                # Parse header: >sp|P68871|HBB_HUMAN or >tr|...\n",
    "                header = line[1:].strip()\n",
    "                parts = header.split(\"|\")\n",
    "                if len(parts) >= 2:\n",
    "                    current_id = parts[1]  # UniProt accession\n",
    "                else:\n",
    "                    current_id = header.split()[0]\n",
    "                seq_chunks = []\n",
    "            else:\n",
    "                seq_chunks.append(line.strip())\n",
    "        \n",
    "        # Flush last\n",
    "        if current_id is not None:\n",
    "            seq = \"\".join(seq_chunks)\n",
    "            uniprot_to_seq[current_id] = seq\n",
    "    \n",
    "    print(f\"      Found {len(uniprot_to_seq)} UniProt‚Üísequence mappings\")\n",
    "    return uniprot_to_seq\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_local_protein_seq(transcript_id: str) -> Optional[str]:\n",
    "    \"\"\"Get protein sequence from local Ensembl cache.\"\"\"\n",
    "    if not transcript_id or ENST_TO_SEQ is None:\n",
    "        return None\n",
    "    # Try versioned, then versionless\n",
    "    return ENST_TO_SEQ.get(transcript_id) or ENST_TO_SEQ.get(transcript_id.split(\".\")[0])\n",
    "\n",
    "\n",
    "def get_uniprot_info(gene_symbol: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Get UniProt ID and sequence from local cache.\n",
    "    Returns: (uniprot_id, sequence)\n",
    "    \"\"\"\n",
    "    if GENE_TO_UNIPROT is None or UNIPROT_TO_SEQ is None:\n",
    "        return None, None\n",
    "    \n",
    "    uniprot_id = GENE_TO_UNIPROT.get(gene_symbol)\n",
    "    if not uniprot_id:\n",
    "        return None, None\n",
    "    \n",
    "    sequence = UNIPROT_TO_SEQ.get(uniprot_id)\n",
    "    return uniprot_id, sequence\n",
    "\n",
    "\n",
    "def initialize_reference_data():\n",
    "    \"\"\"Load all reference data at startup.\"\"\"\n",
    "    global ENST_TO_SEQ, GENE_TO_MANE_ENST, ENST_TO_NM, GENE_TO_UNIPROT, UNIPROT_TO_SEQ\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING LOCAL REFERENCE DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check files exist\n",
    "    required_files = [\n",
    "        (ENSEMBL_GTF, \"Ensembl GTF\"),\n",
    "        (ENSEMBL_PEP, \"Ensembl peptides\"),\n",
    "        (MANE_SUMMARY, \"MANE summary\"),\n",
    "        (UNIPROT_FASTA, \"UniProt FASTA\"),\n",
    "        (UNIPROT_IDMAPPING, \"UniProt ID mapping\")\n",
    "    ]\n",
    "    \n",
    "    for path, name in required_files:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"{name} not found: {path}\")\n",
    "        print(f\"‚úÖ Found: {path}\")\n",
    "    \n",
    "    print(\"\\nüìö Loading Ensembl reference data (GTF + peptides)...\")\n",
    "    ENST_TO_SEQ = build_enst_to_seq(ENSEMBL_GTF, ENSEMBL_PEP)\n",
    "    \n",
    "    print(\"\\nüìö Loading MANE summary...\")\n",
    "    GENE_TO_MANE_ENST, ENST_TO_NM = load_mane_summary(MANE_SUMMARY)\n",
    "    \n",
    "    print(\"\\nüìö Loading UniProt data...\")\n",
    "    GENE_TO_UNIPROT = load_uniprot_id_mapping(UNIPROT_IDMAPPING)\n",
    "    UNIPROT_TO_SEQ = load_uniprot_sequences(UNIPROT_FASTA)\n",
    "    \n",
    "    print(\"\\n‚úÖ All reference data loaded successfully\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: FETCH GNOMAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "def fetch_gnomad_data(gene_symbol: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch gnomAD data for gene\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"STEP 1: FETCHING GNOMAD DATA FOR {gene_symbol}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get gene info\n",
    "    print(f\"\\nüìä Getting gene information...\")\n",
    "    gene_info = get_gene_info(gene_symbol)\n",
    "    \n",
    "    print(f\"   Gene: {gene_symbol}\")\n",
    "    print(f\"   Chromosome: {gene_info['CHROM']}\")\n",
    "    print(f\"   Position: {int(gene_info['chr_start']):,} - {int(gene_info['chr_end']):,}\")\n",
    "    print(f\"   HGNC ID: {gene_info['HGNC_ID']}\")\n",
    "    \n",
    "    # Query gnomAD\n",
    "    print(f\"\\nüîç Querying gnomAD v4...\")\n",
    "    gnomad_file = CACHE_DIR / f\"gnomAD_{gene_symbol}.json\"\n",
    "    \n",
    "    gnomad_df = queryGnomAD(\n",
    "        \"GRCh38\",\n",
    "        gene_info['CHROM'],\n",
    "        int(gene_info['chr_start']),\n",
    "        int(gene_info['chr_end']),\n",
    "        gene_info['HGNC_ID'],\n",
    "        EXTERNAL_TOOLS_CONFIG,\n",
    "        write_dir=str(CACHE_DIR),\n",
    "        use_cache=True,           # ‚Üê Cache results for faster re-runs\n",
    "        gene_symbol=GENE_SYMBOL,  # ‚Üê Better cache naming\n",
    "        parallel=True,            # ‚Üê Run exomes/genomes in parallel (faster)\n",
    "        cleanup=True              # ‚Üê Auto-delete intermediate files (default anyway)\n",
    "    )\n",
    "    \n",
    "    gnomad_df.to_json(gnomad_file)\n",
    "    print(f\"   ‚úÖ Fetched {len(gnomad_df)} variant-transcript pairs\")\n",
    "    print(f\"   ‚úÖ Saved to: {gnomad_file}\")\n",
    "    \n",
    "    return gnomad_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: PARSE PROTEIN-LEVEL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "def parse_hgvsp_notation(hgvsp: str) -> Optional[Dict]:\n",
    "    \"\"\"Parse HGVSp notation to extract ref_aa, position, alt_aa\"\"\"\n",
    "    if pd.isna(hgvsp) or hgvsp == '':\n",
    "        return None\n",
    "    \n",
    "    aa_3to1 = {\n",
    "        'Ala': 'A', 'Arg': 'R', 'Asn': 'N', 'Asp': 'D', 'Cys': 'C',\n",
    "        'Gln': 'Q', 'Glu': 'E', 'Gly': 'G', 'His': 'H', 'Ile': 'I',\n",
    "        'Leu': 'L', 'Lys': 'K', 'Met': 'M', 'Phe': 'F', 'Pro': 'P',\n",
    "        'Ser': 'S', 'Thr': 'T', 'Trp': 'W', 'Tyr': 'Y', 'Val': 'V',\n",
    "        'Ter': '*', 'Stop': '*', 'Sec': 'U', 'Pyl': 'O'\n",
    "    }\n",
    "    \n",
    "    hgvsp_str = str(hgvsp)\n",
    "    if ':p.' in hgvsp_str:\n",
    "        hgvsp_str = 'p.' + hgvsp_str.split(':p.')[1]\n",
    "    \n",
    "    pattern = r'p\\.([A-Z][a-z]{2}|[A-Z\\*])(\\d+)([A-Z][a-z]{2}|[A-Z\\*\\=])'\n",
    "    match = re.match(pattern, hgvsp_str)\n",
    "    \n",
    "    if match:\n",
    "        ref_aa = match.group(1)\n",
    "        pos = int(match.group(2))\n",
    "        alt_aa = match.group(3)\n",
    "        \n",
    "        ref_1letter = aa_3to1.get(ref_aa, ref_aa)\n",
    "        alt_1letter = aa_3to1.get(alt_aa, alt_aa) if alt_aa != '=' else ref_aa\n",
    "        \n",
    "        return {'ref_aa': ref_1letter, 'pos': pos, 'alt_aa': alt_1letter}\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def process_protein_level(gnomad_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process gnomAD data to extract protein-level information using local sequences\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: PROCESSING PROTEIN-LEVEL INFORMATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Filter missense\n",
    "    print(f\"\\nüîç Filtering for missense variants...\")\n",
    "    missense = gnomad_df[\n",
    "        gnomad_df['Consequence'].str.contains('missense', case=False, na=False)\n",
    "    ].copy()\n",
    "    print(f\"   Total missense: {len(missense)}\")\n",
    "    \n",
    "    # Parse HGVSp\n",
    "    print(f\"\\nüß¨ Parsing HGVSp notation...\")\n",
    "    missense['protein_change'] = missense['HGVSp'].apply(parse_hgvsp_notation)\n",
    "    missense['ref_aa'] = missense['protein_change'].apply(lambda x: x['ref_aa'] if x else None)\n",
    "    missense['protein_pos'] = missense['protein_change'].apply(lambda x: x['pos'] if x else None)\n",
    "    missense['alt_aa'] = missense['protein_change'].apply(lambda x: x['alt_aa'] if x else None)\n",
    "    missense['mutation'] = missense.apply(\n",
    "        lambda row: f\"{row['ref_aa']}{row['protein_pos']}{row['alt_aa']}\" if row['ref_aa'] else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    parsed = missense[missense['ref_aa'].notna()].copy()\n",
    "    print(f\"   Parsed: {len(parsed)}/{len(missense)}\")\n",
    "    \n",
    "    # Filter canonical\n",
    "    print(f\"\\nüìå Filtering for canonical transcripts...\")\n",
    "    canonical = parsed[parsed['CANONICAL'] == 'YES'].copy()\n",
    "    print(f\"   Canonical: {len(canonical)}\")\n",
    "    \n",
    "    # Select columns\n",
    "    key_columns = [\n",
    "        'CHROM', 'POS', 'REF', 'ALT',\n",
    "        'SYMBOL', 'Gene', 'Feature', 'Feature_type',\n",
    "        'CANONICAL', 'BIOTYPE',\n",
    "        'ref_aa', 'protein_pos', 'alt_aa', 'mutation',\n",
    "        'HGVSp', 'HGVSc', 'Amino_acids', 'Codons',\n",
    "        'AF', 'AC', 'AN', 'IMPACT', 'Consequence',\n",
    "        'MANE_SELECT', 'EXON'\n",
    "    ]\n",
    "    \n",
    "    available_columns = [col for col in key_columns if col in canonical.columns]\n",
    "    result = canonical[available_columns].copy()\n",
    "    \n",
    "    # Attach sequences from local cache\n",
    "    print(f\"\\nüî¨ Attaching protein sequences from local Ensembl cache...\")\n",
    "    result['protein_seq'] = result['Feature'].apply(get_local_protein_seq)\n",
    "    \n",
    "    seqs_fetched = result['protein_seq'].notna().sum()\n",
    "    print(f\"üìä Sequences attached: {seqs_fetched}/{len(result)}\")\n",
    "    \n",
    "    if seqs_fetched == 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: No sequences found in local cache!\")\n",
    "    \n",
    "    # Verify sequences\n",
    "    print(f\"\\n‚úÖ Verifying sequences...\")\n",
    "    def verify_match(row):\n",
    "        if pd.isna(row['protein_seq']) or pd.isna(row['protein_pos']):\n",
    "            return None\n",
    "        try:\n",
    "            actual = row['protein_seq'][row['protein_pos'] - 1]\n",
    "            expected = row['ref_aa']\n",
    "            return actual == expected\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    result['seq_verified'] = result.apply(verify_match, axis=1)\n",
    "    verified = result['seq_verified'].sum()\n",
    "    print(f\"   Verified: {verified}/{len(result)}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    print(f\"\\nüîÑ Removing genomic duplicates...\")\n",
    "    result_unique = result.drop_duplicates(subset=['CHROM', 'POS', 'REF', 'ALT']).copy()\n",
    "    print(f\"   Before: {len(result)}\")\n",
    "    print(f\"   After: {len(result_unique)}\")\n",
    "    \n",
    "    return result_unique\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: INTERNAL VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_internal(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Comprehensive internal validation\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: INTERNAL VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Check required columns\n",
    "    print(f\"\\n1Ô∏è‚É£  Required columns...\")\n",
    "    required = ['CHROM', 'POS', 'REF', 'ALT', 'ref_aa', 'protein_pos', 'alt_aa', 'mutation']\n",
    "    missing = [col for col in required if col not in df.columns]\n",
    "    if missing:\n",
    "        errors.append(f\"Missing columns: {missing}\")\n",
    "        print(f\"   ‚ùå Missing: {missing}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All present\")\n",
    "    \n",
    "    # Check nulls\n",
    "    print(f\"\\n2Ô∏è‚É£  Null values...\")\n",
    "    for col in ['ref_aa', 'protein_pos', 'alt_aa']:\n",
    "        if col in df.columns:\n",
    "            null_count = df[col].isna().sum()\n",
    "            if null_count > 0:\n",
    "                errors.append(f\"{col} has {null_count} nulls\")\n",
    "                print(f\"   ‚ùå {col}: {null_count} nulls\")\n",
    "            else:\n",
    "                print(f\"   ‚úÖ {col}: no nulls\")\n",
    "    \n",
    "    # Check protein_seq separately\n",
    "    if 'protein_seq' in df.columns:\n",
    "        null_count = df['protein_seq'].isna().sum()\n",
    "        if null_count > 0:\n",
    "            warnings.append(f\"protein_seq has {null_count} nulls (sequence not in local cache)\")\n",
    "            print(f\"   ‚ö†Ô∏è  protein_seq: {null_count} nulls (not in cache)\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ protein_seq: no nulls\")\n",
    "    \n",
    "    # Check amino acids\n",
    "    print(f\"\\n3Ô∏è‚É£  Amino acid validity...\")\n",
    "    valid_aas = set('ACDEFGHIKLMNPQRSTVWY*')\n",
    "    invalid_ref = df[~df['ref_aa'].isin(valid_aas)]\n",
    "    invalid_alt = df[~df['alt_aa'].isin(valid_aas)]\n",
    "    \n",
    "    if len(invalid_ref) > 0:\n",
    "        errors.append(f\"{len(invalid_ref)} invalid ref_aa\")\n",
    "        print(f\"   ‚ùå Invalid ref_aa: {len(invalid_ref)}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All ref_aa valid\")\n",
    "    \n",
    "    if len(invalid_alt) > 0:\n",
    "        errors.append(f\"{len(invalid_alt)} invalid alt_aa\")\n",
    "        print(f\"   ‚ùå Invalid alt_aa: {len(invalid_alt)}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All alt_aa valid\")\n",
    "    \n",
    "    # Verify sequences\n",
    "    print(f\"\\n4Ô∏è‚É£  Sequence verification...\")\n",
    "    if 'seq_verified' in df.columns:\n",
    "        verified = df['seq_verified'].sum()\n",
    "        total_with_seq = df['protein_seq'].notna().sum()\n",
    "        print(f\"   Verified: {verified}/{total_with_seq} (with sequences)\")\n",
    "        \n",
    "        if total_with_seq == 0:\n",
    "            warnings.append(\"No sequences available for verification\")\n",
    "    \n",
    "    # Check duplicates\n",
    "    print(f\"\\n5Ô∏è‚É£  Duplicates...\")\n",
    "    genomic_dups = df[df.duplicated(subset=['CHROM', 'POS', 'REF', 'ALT'], keep=False)]\n",
    "    if len(genomic_dups) > 0:\n",
    "        errors.append(f\"{len(genomic_dups)} genomic duplicates\")\n",
    "        print(f\"   ‚ùå Genomic: {len(genomic_dups)}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No genomic duplicates\")\n",
    "    \n",
    "    protein_dups = df[df.duplicated(subset=['Feature', 'mutation'], keep=False)]\n",
    "    if len(protein_dups) > 0:\n",
    "        warnings.append(f\"{len(protein_dups)} protein duplicates (different nucleotides‚Üísame AA)\")\n",
    "        print(f\"   ‚ö†Ô∏è  Protein: {len(protein_dups)} (OK - degeneracy)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No protein duplicates\")\n",
    "    \n",
    "    # Check AF range\n",
    "    print(f\"\\n6Ô∏è‚É£  Allele frequencies...\")\n",
    "    if 'AF' in df.columns:\n",
    "        af_series = pd.to_numeric(df['AF'], errors='coerce')\n",
    "        invalid_af = df[(af_series < 0) | (af_series > 1)]\n",
    "        if len(invalid_af) > 0:\n",
    "            errors.append(f\"{len(invalid_af)} AF out of range\")\n",
    "            print(f\"   ‚ùå Invalid AF: {len(invalid_af)}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All AF in [0,1]\")\n",
    "    \n",
    "    return {'errors': errors, 'warnings': warnings}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: EXTERNAL VALIDATION (ALL OFFLINE)\n",
    "# ============================================================================\n",
    "\n",
    "def validate_external(df: pd.DataFrame, gene_symbol: str) -> Dict:\n",
    "    \"\"\"Validate against local UniProt and MANE references (NO online queries)\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: EXTERNAL VALIDATION (ALL OFFLINE)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    your_transcript = df['Feature'].iloc[0]\n",
    "    your_sequence = df['protein_seq'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nüìÇ Your data:\")\n",
    "    print(f\"   Transcript: {your_transcript}\")\n",
    "    \n",
    "    if your_sequence is None:\n",
    "        print(f\"   ‚ö†Ô∏è  Sequence: NOT AVAILABLE (not in cache)\")\n",
    "        print(f\"   Variants: {len(df)}\")\n",
    "    else:\n",
    "        print(f\"   Sequence: {len(your_sequence)} aa\")\n",
    "        print(f\"   Variants: {len(df)}\")\n",
    "    \n",
    "    issues = []\n",
    "    all_sequences = {}\n",
    "    \n",
    "    if your_sequence:\n",
    "        all_sequences['Your_Data'] = your_sequence\n",
    "    \n",
    "    # Query UniProt LOCALLY\n",
    "    print(f\"\\nüî¨ Querying UniProt (LOCAL)...\")\n",
    "    uniprot_id, uniprot_seq = get_uniprot_info(gene_symbol)\n",
    "    \n",
    "    if uniprot_id and uniprot_seq:\n",
    "        print(f\"   ‚úÖ UniProt: {uniprot_id}\")\n",
    "        print(f\"   Length: {len(uniprot_seq)} aa\")\n",
    "        \n",
    "        all_sequences['UniProt'] = uniprot_seq\n",
    "        \n",
    "        if your_sequence:\n",
    "            if uniprot_seq == your_sequence:\n",
    "                print(f\"   ‚úÖ Sequence MATCHES UniProt (100%)\")\n",
    "            else:\n",
    "                issues.append(f\"Sequence differs from UniProt {uniprot_id}\")\n",
    "                print(f\"   ‚ùå Sequence DIFFERS from UniProt\")\n",
    "                print(f\"      Your length: {len(your_sequence)}\")\n",
    "                print(f\"      UniProt length: {len(uniprot_seq)}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Cannot compare - your sequence not available\")\n",
    "    else:\n",
    "        issues.append(f\"No UniProt entry found for {gene_symbol}\")\n",
    "        print(f\"   ‚ùå No UniProt entry found\")\n",
    "        uniprot_id = None\n",
    "    \n",
    "    # Check MANE Select locally\n",
    "    print(f\"\\nüìã Checking MANE Select (LOCAL)...\")\n",
    "    your_base = your_transcript.split(\".\")[0]\n",
    "    \n",
    "    mane_enst = GENE_TO_MANE_ENST.get(gene_symbol)\n",
    "    if mane_enst:\n",
    "        print(f\"   ‚úÖ MANE Select ENST for {gene_symbol}: {mane_enst}\")\n",
    "        \n",
    "        if your_base == mane_enst:\n",
    "            print(f\"   ‚úÖ Using MANE Select transcript\")\n",
    "        else:\n",
    "            issues.append(f\"Transcript {your_base} != MANE Select {mane_enst}\")\n",
    "            print(f\"   ‚ö†Ô∏è  Different from MANE Select transcript\")\n",
    "        \n",
    "        # Get corresponding RefSeq\n",
    "        refseq_id = ENST_TO_NM.get(mane_enst)\n",
    "        if refseq_id:\n",
    "            print(f\"   ‚úÖ Corresponding RefSeq (NM): {refseq_id}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  No NM mapping found for {mane_enst}\")\n",
    "        \n",
    "        # Use MANE ENST sequence for validation\n",
    "        mane_seq = get_local_protein_seq(mane_enst)\n",
    "        if mane_seq:\n",
    "            all_sequences[\"MANE_ENST\"] = mane_seq\n",
    "            print(f\"   ‚úÖ MANE Select sequence: {len(mane_seq)} aa\")\n",
    "            \n",
    "            if your_sequence:\n",
    "                if mane_seq == your_sequence:\n",
    "                    print(f\"   ‚úÖ Your sequence MATCHES MANE Select\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Your sequence DIFFERS from MANE Select\")\n",
    "                    issues.append(\"Sequence differs from MANE Select transcript\")\n",
    "            else:\n",
    "                print(f\"   üí° Using MANE Select sequence as reference\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  No sequence found for MANE ENST {mane_enst} in cache\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  No MANE Select entry for {gene_symbol}\")\n",
    "        issues.append(\"No MANE Select entry in MANE summary\")\n",
    "    \n",
    "    # Compare all sequences\n",
    "    if len(all_sequences) > 1:\n",
    "        print(f\"\\nüîç Cross-database sequence comparison:\")\n",
    "        unique_seqs = set(all_sequences.values())\n",
    "        if len(unique_seqs) == 1:\n",
    "            print(f\"   ‚úÖ ALL SEQUENCES IDENTICAL across {len(all_sequences)} databases\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Found {len(unique_seqs)} different sequences:\")\n",
    "            for db_name, seq in all_sequences.items():\n",
    "                print(f\"      {db_name}: {len(seq)} aa\")\n",
    "    elif len(all_sequences) == 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  No sequences available for comparison\")\n",
    "        issues.append(\"No sequences available for validation\")\n",
    "    \n",
    "    # Validate gnomAD annotations\n",
    "    print(f\"\\nüîç Validating gnomAD annotations...\")\n",
    "    \n",
    "    # Check amino acid consistency\n",
    "    mismatches = 0\n",
    "    if 'Amino_acids' in df.columns:\n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.notna(row['Amino_acids']):\n",
    "                parts = row['Amino_acids'].split('/')\n",
    "                if len(parts) == 2:\n",
    "                    if parts[0] != row['ref_aa'] or parts[1] != row['alt_aa']:\n",
    "                        mismatches += 1\n",
    "    \n",
    "    if mismatches == 0:\n",
    "        print(f\"   ‚úÖ Amino acids consistent ({len(df)} variants)\")\n",
    "    else:\n",
    "        issues.append(f\"{mismatches} amino acid mismatches\")\n",
    "        print(f\"   ‚ùå {mismatches} mismatches\")\n",
    "    \n",
    "    # Check consequences\n",
    "    if 'Consequence' in df.columns:\n",
    "        all_missense = df['Consequence'].str.contains('missense', case=False, na=False).all()\n",
    "        if all_missense:\n",
    "            print(f\"   ‚úÖ All variants are missense\")\n",
    "        else:\n",
    "            issues.append(\"Non-missense variants found\")\n",
    "            print(f\"   ‚ùå Non-missense found\")\n",
    "    \n",
    "    return {\n",
    "        'issues': issues,\n",
    "        'uniprot_id': uniprot_id,\n",
    "        'all_sequences': all_sequences\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: ADD STRUCTURE IDS\n",
    "# ============================================================================\n",
    "\n",
    "def add_structure_ids(df: pd.DataFrame, uniprot_id: Optional[str]) -> pd.DataFrame:\n",
    "    \"\"\"Add UniProt and AlphaFold IDs\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: ADDING STRUCTURE IDS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if uniprot_id:\n",
    "        df['uniprot_id'] = uniprot_id\n",
    "        df['alphafold_id'] = f\"AF-{uniprot_id}-F1\"\n",
    "        df['alphafold_pdb_url'] = f\"https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v4.pdb\"\n",
    "        \n",
    "        print(f\"\\n‚úÖ Added structure IDs:\")\n",
    "        print(f\"   UniProt: {uniprot_id}\")\n",
    "        print(f\"   AlphaFold: AF-{uniprot_id}-F1\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No UniProt ID available\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: GENERATE REPORT\n",
    "# ============================================================================\n",
    "\n",
    "def generate_report(df: pd.DataFrame, internal_results: Dict, external_results: Dict, report_path: Path):\n",
    "    \"\"\"Generate validation report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 6: GENERATING VALIDATION REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"HBB VARIANT VALIDATION REPORT (Fully Offline)\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Gene: HBB (Hemoglobin subunit beta)\\n\")\n",
    "        f.write(f\"Total variants: {len(df)}\\n\")\n",
    "        f.write(f\"Transcript: {df['Feature'].iloc[0]}\\n\")\n",
    "        \n",
    "        first_seq = df['protein_seq'].iloc[0]\n",
    "        if first_seq:\n",
    "            f.write(f\"Protein sequence length: {len(first_seq)} aa\\n\\n\")\n",
    "        else:\n",
    "            f.write(f\"Protein sequence: NOT AVAILABLE (not in local cache)\\n\\n\")\n",
    "        \n",
    "        f.write(\"INTERNAL VALIDATION\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        errors = internal_results['errors']\n",
    "        warnings = internal_results['warnings']\n",
    "        \n",
    "        if errors:\n",
    "            f.write(f\"‚ùå ERRORS ({len(errors)}):\\n\")\n",
    "            for err in errors:\n",
    "                f.write(f\"   - {err}\\n\")\n",
    "        else:\n",
    "            f.write(\"‚úÖ No errors\\n\")\n",
    "        \n",
    "        if warnings:\n",
    "            f.write(f\"\\n‚ö†Ô∏è  WARNINGS ({len(warnings)}):\\n\")\n",
    "            for warn in warnings:\n",
    "                f.write(f\"   - {warn}\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n‚úÖ No warnings\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nEXTERNAL VALIDATION (ALL OFFLINE)\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        issues = external_results['issues']\n",
    "        uniprot_id = external_results['uniprot_id']\n",
    "        all_sequences = external_results.get('all_sequences', {})\n",
    "        \n",
    "        if uniprot_id:\n",
    "            f.write(f\"‚úÖ UniProt ID: {uniprot_id} (from local cache)\\n\")\n",
    "            f.write(f\"‚úÖ AlphaFold ID: AF-{uniprot_id}-F1\\n\")\n",
    "        \n",
    "        if all_sequences:\n",
    "            f.write(f\"\\nSequence Cross-Validation (all sources local):\\n\")\n",
    "            for db_name, seq in all_sequences.items():\n",
    "                f.write(f\"   {db_name}: {len(seq)} aa\\n\")\n",
    "            \n",
    "            unique_seqs = set(all_sequences.values())\n",
    "            if len(unique_seqs) == 1:\n",
    "                f.write(f\"   ‚úÖ ALL IDENTICAL\\n\")\n",
    "            else:\n",
    "                f.write(f\"   ‚ö†Ô∏è  {len(unique_seqs)} different sequences detected\\n\")\n",
    "        \n",
    "        if issues:\n",
    "            f.write(f\"\\n‚ö†Ô∏è  ISSUES ({len(issues)}):\\n\")\n",
    "            for issue in issues:\n",
    "                f.write(f\"   - {issue}\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n‚úÖ No issues\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nVARIANT STATISTICS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(f\"Position range: {df['protein_pos'].min()} - {df['protein_pos'].max()}\\n\")\n",
    "        f.write(f\"Unique positions: {df['protein_pos'].nunique()}\\n\")\n",
    "        f.write(f\"Unique mutations: {df['mutation'].nunique()}\\n\")\n",
    "        \n",
    "        if 'AF' in df.columns:\n",
    "            af_series = pd.to_numeric(df['AF'], errors='coerce')\n",
    "            f.write(f\"\\nAllele Frequency Distribution:\\n\")\n",
    "            f.write(f\"   Ultra-rare (< 0.00001): {(af_series < 0.00001).sum()}\\n\")\n",
    "            f.write(f\"   Rare (0.00001-0.001): {((af_series >= 0.00001) & (af_series < 0.001)).sum()}\\n\")\n",
    "            f.write(f\"   Common (> 0.01): {(af_series >= 0.01).sum()}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nFINAL VERDICT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        critical_errors = [e for e in errors if 'null' not in e.lower()]\n",
    "        \n",
    "        if not critical_errors and not issues:\n",
    "            f.write(\"üéâ ‚úÖ DATA IS VALIDATED (FULLY OFFLINE)\\n\\n\")\n",
    "            f.write(\"Your data is:\\n\")\n",
    "            f.write(\"  ‚úì Internally consistent\\n\")\n",
    "            if all_sequences:\n",
    "                f.write(\"  ‚úì Externally validated (\")\n",
    "                f.write(\", \".join(all_sequences.keys()))\n",
    "                f.write(\" - all from local caches)\\n\")\n",
    "            f.write(\"  ‚úì Ready for production use\\n\")\n",
    "            f.write(\"  ‚úì Safe for downstream analysis\\n\")\n",
    "            if warnings:\n",
    "                f.write(f\"\\nNote: {len(warnings)} warnings (see above)\\n\")\n",
    "        else:\n",
    "            f.write(\"‚ö†Ô∏è  VALIDATION ISSUES DETECTED\\n\\n\")\n",
    "            f.write(\"Please review errors and issues above.\\n\")\n",
    "    \n",
    "    print(f\"‚úÖ Report saved: {report_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HBB COMPLETE VALIDATION PIPELINE (FULLY OFFLINE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nGene: {GENE_SYMBOL}\")\n",
    "    print(f\"Output: {OUTPUT_CSV}\")\n",
    "    print(f\"\\nüåê Network requirements: NONE (all validation is offline)\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Initialize reference data\n",
    "        initialize_reference_data()\n",
    "        \n",
    "        # Step 1: Fetch data\n",
    "        gnomad_df = fetch_gnomad_data(GENE_SYMBOL)\n",
    "        \n",
    "        # Step 2: Process\n",
    "        df = process_protein_level(gnomad_df)\n",
    "        \n",
    "        # Step 3: Internal validation\n",
    "        internal_results = validate_internal(df)\n",
    "        \n",
    "        # Step 4: External validation\n",
    "        external_results = validate_external(df, GENE_SYMBOL)\n",
    "        \n",
    "        # Step 5: Add structure IDs\n",
    "        df = add_structure_ids(df, external_results['uniprot_id'])\n",
    "        \n",
    "        # Step 6: Generate report\n",
    "        generate_report(df, internal_results, external_results, VALIDATION_REPORT)\n",
    "        \n",
    "        # Save data\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SAVING FINAL DATA\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(f\"‚úÖ CSV: {OUTPUT_CSV}\")\n",
    "        \n",
    "        df.to_json(OUTPUT_JSON, orient='records', indent=2)\n",
    "        print(f\"‚úÖ JSON: {OUTPUT_JSON}\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PIPELINE COMPLETE (FULLY OFFLINE)\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        errors = internal_results['errors']\n",
    "        warnings = internal_results['warnings']\n",
    "        issues = external_results['issues']\n",
    "        all_sequences = external_results.get('all_sequences', {})\n",
    "        \n",
    "        critical_errors = [e for e in errors if 'null' not in e.lower()]\n",
    "        \n",
    "        if not critical_errors and not issues:\n",
    "            print(\"\\nüéâ ‚úÖ SUCCESS - DATA IS VALIDATED!\")\n",
    "            print(f\"\\n   Total variants: {len(df)}\")\n",
    "            print(f\"   UniProt ID: {external_results['uniprot_id']} (from local cache)\")\n",
    "            \n",
    "            if len(all_sequences) > 1:\n",
    "                unique_seqs = set(all_sequences.values())\n",
    "                if len(unique_seqs) == 1:\n",
    "                    print(f\"   ‚úÖ Sequences validated across {len(all_sequences)} databases (all local)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  {len(unique_seqs)} different sequences found\")\n",
    "            \n",
    "            if warnings:\n",
    "                print(f\"\\n   ‚ö†Ô∏è  {len(warnings)} warnings (non-critical)\")\n",
    "            \n",
    "            print(\"\\n   ‚úÖ SAFE TO PROCEED WITH ANALYSIS\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  VALIDATION COMPLETED WITH ISSUES\")\n",
    "            print(f\"\\n   Critical errors: {len(critical_errors)}\")\n",
    "            print(f\"   Warnings: {len(warnings)}\")\n",
    "            print(f\"   Issues: {len(issues)}\")\n",
    "            print(f\"\\n   See report: {VALIDATION_REPORT}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        \n",
    "        # Display sample\n",
    "        print(\"üìä Sample output:\")\n",
    "        display_cols = ['CHROM', 'POS', 'mutation', 'AF', 'uniprot_id', 'alphafold_id']\n",
    "        available = [c for c in display_cols if c in df.columns]\n",
    "        print(df[available].head(10).to_string(index=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå PIPELINE FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = main()\n",
    "    exit(0 if success else 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
