{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java version being used:\n",
      "java version \"21.0.9\" 2025-10-21 LTS\n",
      "Java(TM) SE Runtime Environment (build 21.0.9+7-LTS-338)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 21.0.9+7-LTS-338, mixed mode, sharing)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using GATK jar /ocean/projects/cis250266p/barazand/proteins/gnomAD/gatk-4.6.1.0/gatk-package-4.6.1.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /ocean/projects/cis250266p/barazand/proteins/gnomAD/gatk-4.6.1.0/gatk-package-4.6.1.0-local.jar SelectVariants -V .cache/clinvar_20181217.vcf.gz -L 11:5225464-5227071 --exclude-filtered --output .cache/ClinVar_selectvariants_chr11:5225464-5227071.vcf\n",
      "13:01:46.538 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/ocean/projects/cis250266p/barazand/proteins/gnomAD/gatk-4.6.1.0/gatk-package-4.6.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "SLF4J(W): Class path contains multiple SLF4J providers.\n",
      "SLF4J(W): Found provider [org.apache.logging.slf4j.SLF4JServiceProvider@935493d]\n",
      "SLF4J(W): Found provider [ch.qos.logback.classic.spi.LogbackServiceProvider@9b367c8]\n",
      "SLF4J(W): See https://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J(I): Actual provider is of type [org.apache.logging.slf4j.SLF4JServiceProvider@935493d]\n",
      "13:01:46.805 INFO  SelectVariants - ------------------------------------------------------------\n",
      "13:01:46.809 INFO  SelectVariants - The Genome Analysis Toolkit (GATK) v4.6.1.0\n",
      "13:01:46.821 INFO  SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/\n",
      "13:01:46.821 INFO  SelectVariants - Executing as barazand@r007.ib.bridges2.psc.edu on Linux v4.18.0-553.22.1.el8_10.x86_64 amd64\n",
      "13:01:46.823 INFO  SelectVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v21.0.9+7-LTS-338\n",
      "13:01:46.823 INFO  SelectVariants - Start Date/Time: December 8, 2025, 1:01:46‚ÄØPM EST\n",
      "13:01:46.824 INFO  SelectVariants - ------------------------------------------------------------\n",
      "13:01:46.824 INFO  SelectVariants - ------------------------------------------------------------\n",
      "13:01:46.826 INFO  SelectVariants - HTSJDK Version: 4.1.3\n",
      "13:01:46.826 INFO  SelectVariants - Picard Version: 3.3.0\n",
      "13:01:46.826 INFO  SelectVariants - Built for Spark Version: 3.5.0\n",
      "13:01:46.829 INFO  SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n",
      "13:01:46.831 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n",
      "13:01:46.831 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n",
      "13:01:46.831 INFO  SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n",
      "13:01:46.831 INFO  SelectVariants - Deflater: IntelDeflater\n",
      "13:01:46.832 INFO  SelectVariants - Inflater: IntelInflater\n",
      "13:01:46.832 INFO  SelectVariants - GCS max retries/reopens: 20\n",
      "13:01:46.832 INFO  SelectVariants - Requester pays: disabled\n",
      "13:01:46.832 INFO  SelectVariants - Initializing engine\n",
      "13:01:47.072 INFO  FeatureManager - Using codec VCFCodec to read file file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz\n",
      "13:01:47.316 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:47.455 WARN  IndexUtils - Index file /ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz.tbi is out of date (index older than input file). Use IndexFeatureFile to make a new index.\n",
      "13:01:47.472 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:47.541 WARN  IndexUtils - Index file /ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz.tbi is out of date (index older than input file). Use IndexFeatureFile to make a new index.\n",
      "13:01:47.562 WARN  GenomeLocParser - The available sequence dictionary does not contain a sequence length for contig (11). Skipping validation of the genome loc end coordinate (5227071).\n",
      "13:01:47.563 INFO  IntervalArgumentCollection - Processing 1608 bp from intervals\n",
      "13:01:47.564 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:47.624 WARN  IndexUtils - Index file /ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz.tbi is out of date (index older than input file). Use IndexFeatureFile to make a new index.\n",
      "13:01:47.632 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:47.698 WARN  IndexUtils - Index file /ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz.tbi is out of date (index older than input file). Use IndexFeatureFile to make a new index.\n",
      "13:01:47.706 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:47.774 WARN  IndexUtils - Index file /ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz.tbi is out of date (index older than input file). Use IndexFeatureFile to make a new index.\n",
      "13:01:47.790 INFO  SelectVariants - Done initializing engine\n",
      "13:01:47.842 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:47.949 WARN  IndexUtils - Index file /ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz.tbi is out of date (index older than input file). Use IndexFeatureFile to make a new index.\n",
      "13:01:47.950 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:48.001 WARN  IndexUtils - Index file /ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/clinvar_20181217.vcf.gz.tbi is out of date (index older than input file). Use IndexFeatureFile to make a new index.\n",
      "13:01:48.041 INFO  ProgressMeter - Starting traversal\n",
      "13:01:48.042 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute\n",
      "13:01:48.356 INFO  ProgressMeter -             unmapped              0.0                   593         118600.0\n",
      "13:01:48.357 INFO  ProgressMeter - Traversal complete. Processed 593 total variants in 0.0 minutes.\n",
      "13:01:48.367 INFO  SelectVariants - Shutting down engine\n",
      "[December 8, 2025, 1:01:48‚ÄØPM EST] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.03 minutes.\n",
      "Runtime.totalMemory()=100519936\n",
      "Using GATK jar /ocean/projects/cis250266p/barazand/proteins/gnomAD/gatk-4.6.1.0/gatk-package-4.6.1.0-local.jar\n",
      "Running:\n",
      "    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /ocean/projects/cis250266p/barazand/proteins/gnomAD/gatk-4.6.1.0/gatk-package-4.6.1.0-local.jar VariantsToTable -V .cache/ClinVar_selectvariants_chr11:5225464-5227071.vcf -O .cache/ClinVar_selectvariants_chr11:5225464-5227071.tsv\n",
      "13:01:53.028 INFO  NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/ocean/projects/cis250266p/barazand/proteins/gnomAD/gatk-4.6.1.0/gatk-package-4.6.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so\n",
      "SLF4J(W): Class path contains multiple SLF4J providers.\n",
      "SLF4J(W): Found provider [org.apache.logging.slf4j.SLF4JServiceProvider@79ca7bea]\n",
      "SLF4J(W): Found provider [ch.qos.logback.classic.spi.LogbackServiceProvider@54f6b629]\n",
      "SLF4J(W): See https://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J(I): Actual provider is of type [org.apache.logging.slf4j.SLF4JServiceProvider@79ca7bea]\n",
      "13:01:53.309 INFO  VariantsToTable - ------------------------------------------------------------\n",
      "13:01:53.313 INFO  VariantsToTable - The Genome Analysis Toolkit (GATK) v4.6.1.0\n",
      "13:01:53.326 INFO  VariantsToTable - For support and documentation go to https://software.broadinstitute.org/gatk/\n",
      "13:01:53.326 INFO  VariantsToTable - Executing as barazand@r007.ib.bridges2.psc.edu on Linux v4.18.0-553.22.1.el8_10.x86_64 amd64\n",
      "13:01:53.328 INFO  VariantsToTable - Java runtime: Java HotSpot(TM) 64-Bit Server VM v21.0.9+7-LTS-338\n",
      "13:01:53.328 INFO  VariantsToTable - Start Date/Time: December 8, 2025, 1:01:52‚ÄØPM EST\n",
      "13:01:53.328 INFO  VariantsToTable - ------------------------------------------------------------\n",
      "13:01:53.329 INFO  VariantsToTable - ------------------------------------------------------------\n",
      "13:01:53.330 INFO  VariantsToTable - HTSJDK Version: 4.1.3\n",
      "13:01:53.330 INFO  VariantsToTable - Picard Version: 3.3.0\n",
      "13:01:53.330 INFO  VariantsToTable - Built for Spark Version: 3.5.0\n",
      "13:01:53.336 INFO  VariantsToTable - HTSJDK Defaults.COMPRESSION_LEVEL : 2\n",
      "13:01:53.337 INFO  VariantsToTable - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false\n",
      "13:01:53.337 INFO  VariantsToTable - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true\n",
      "13:01:53.337 INFO  VariantsToTable - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false\n",
      "13:01:53.337 INFO  VariantsToTable - Deflater: IntelDeflater\n",
      "13:01:53.338 INFO  VariantsToTable - Inflater: IntelInflater\n",
      "13:01:53.338 INFO  VariantsToTable - GCS max retries/reopens: 20\n",
      "13:01:53.338 INFO  VariantsToTable - Requester pays: disabled\n",
      "13:01:53.339 INFO  VariantsToTable - Initializing engine\n",
      "13:01:53.574 INFO  FeatureManager - Using codec VCFCodec to read file file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/ClinVar_selectvariants_chr11:5225464-5227071.vcf\n",
      "13:01:53.635 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/ClinVar_selectvariants_chr11:5225464-5227071.vcf\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:53.640 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/ClinVar_selectvariants_chr11:5225464-5227071.vcf\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:53.643 WARN  IndexUtils - Feature file \"file:///ocean/projects/cis250266p/barazand/proteins/gnomAD/variant_utils-main/.cache/ClinVar_selectvariants_chr11:5225464-5227071.vcf\" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file\n",
      "13:01:53.655 INFO  VariantsToTable - Done initializing engine\n",
      "13:01:53.670 WARN  VariantsToTable - No fields were specified. All fields declared in the VCF header will be included in the output table.\n",
      "13:01:53.671 WARN  VariantsToTable - Allele-specific fields will only be split if splitting multi-allelic variants is specified (`--split-multi-allelic` or `-SMA`\n",
      "13:01:53.672 INFO  ProgressMeter - Starting traversal\n",
      "13:01:53.673 INFO  ProgressMeter -        Current Locus  Elapsed Minutes    Variants Processed  Variants/Minute\n",
      "13:01:53.981 INFO  ProgressMeter -             unmapped              0.0                   593         115895.8\n",
      "13:01:53.982 INFO  ProgressMeter - Traversal complete. Processed 593 total variants in 0.0 minutes.\n",
      "13:01:53.982 INFO  VariantsToTable - Shutting down engine\n",
      "[December 8, 2025, 1:01:53‚ÄØPM EST] org.broadinstitute.hellbender.tools.walkers.variantutils.VariantsToTable done. Elapsed time: 0.02 minutes.\n",
      "Runtime.totalMemory()=63418368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_queryClinVarVCF() passed successfully!\n",
      "test_queryGnomAD() passed successfully!\n"
     ]
    }
   ],
   "source": [
    "from variant_utils.get_gene_info import get_gene_info\n",
    "from variant_utils.clinvar_utils import queryClinVarVCF\n",
    "from variant_utils.gnomad_utils import queryGnomAD\n",
    "from variant_utils.spliceAI_utils import querySpliceAI\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set Java 21 environment BEFORE any imports that might use Java\n",
    "os.environ['JAVA_HOME'] = '/jet/home/barazand/NEWOCEAN/java/jdk-21.0.9'\n",
    "os.environ['PATH'] = f\"/jet/home/barazand/NEWOCEAN/java/jdk-21.0.9/bin:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "# Verify it's set correctly\n",
    "import subprocess\n",
    "result = subprocess.run(['java', '-version'], capture_output=True, text=True)\n",
    "print(\"Java version being used:\")\n",
    "print(result.stderr)  # java -version outputs to stderr\n",
    "\n",
    "def test_get_gene_info():\n",
    "    get_gene_info('SRY').to_json(\".cache/SRY.json\")\n",
    "\n",
    "def test_queryClinVarVCF():\n",
    "    brca1_info = get_gene_info(\"BRCA1\")\n",
    "    sry_info = get_gene_info(\"SRY\")\n",
    "    pik3ca_info = get_gene_info(\"HBB\")\n",
    "    # set destination to save/reload ClinVar\n",
    "    cache_dir = Path(\".cache\")\n",
    "    cache_dir.mkdir(exist_ok=True)\n",
    "    # download ClinVar release (e.g., 2018-12-17) if file is not present\n",
    "    clinvar_filepath = cache_dir / \"clinvar_20181217.vcf.gz\"\n",
    "    idx_filepath = cache_dir / \"clinvar_20181217.vcf.gz.tbi\"\n",
    "    if not clinvar_filepath.exists():\n",
    "        urllib.request.urlretrieve(f\"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/weekly/clinvar_20181217.vcf.gz\",str(clinvar_filepath))\n",
    "    if not idx_filepath.exists():\n",
    "        urllib.request.urlretrieve(f\"https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/weekly/clinvar_20181217.vcf.gz.tbi\",str(idx_filepath))\n",
    "\n",
    "    # brca1_clinvar_variants = queryClinVarVCF(str(clinvar_filepath), brca1_info.CHROM, brca1_info.chr_start, brca1_info.chr_end, \"external_tools.json\",write_dir=\".cache\")\n",
    "    # brca1_clinvar_variants.to_json(\".cache/BRCA1_clinvar.json\")\n",
    "\n",
    "    # brca1_clinvar_variants = queryClinVarVCF(str(clinvar_filepath), sry_info.CHROM, sry_info.chr_start, sry_info.chr_end, \"external_tools.json\",write_dir=\".cache\")\n",
    "    # brca1_clinvar_variants.to_json(\".cache/SRY_clinvar.json\")\n",
    "\n",
    "    pik3ca_clinvar_variants = queryClinVarVCF(str(clinvar_filepath), pik3ca_info.CHROM, pik3ca_info.chr_start, pik3ca_info.chr_end, \"external_tools.json\",write_dir=\".cache\")\n",
    "    pik3ca_clinvar_variants.to_json(\".cache/HBB_clinvar.json\")\n",
    "\n",
    "\n",
    "def test_queryGnomAD():\n",
    "    gnomAD_tst_out = queryGnomAD('GRCh38', 'Y', 0, 100000000,\"HGNC:11311\",'external_tools.json',write_dir=\".cache\")\n",
    "    gnomAD_tst_out.to_json(\".cache/gnomAD_test.json\")\n",
    "\n",
    "\n",
    "# try:\n",
    "#     test_get_gene_info()\n",
    "#     print(\"test_get_gene_info() passed successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"test_get_gene_info() failed. Error: {e}\")\n",
    "\n",
    "try:\n",
    "    test_queryClinVarVCF()\n",
    "    print(\"test_queryClinVarVCF() passed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"test_queryClinVarVCF() failed. Error: {e}\")\n",
    "\n",
    "try:\n",
    "    # test_queryGnomAD()\n",
    "    sry_info = get_gene_info(\"SRY\")\n",
    "    brca1_info = get_gene_info(\"BRCA1\")\n",
    "    pik3ca_info = get_gene_info(\"HBB\")\n",
    "    # sry_gnomad_variants = queryGnomAD(\"GRCh38\",sry_info.CHROM, sry_info.chr_start, sry_info.chr_end, sry_info.HGNC_ID,\"external_tools.json\", write_dir=\".cache\")\n",
    "    # sry_gnomad_variants.to_json(\".cache/gnomAD_test_SRY.json\")\n",
    "\n",
    "    # sry_gnomad_variants = queryGnomAD(\"GRCh38\",pik3ca_info.CHROM, pik3ca_info.chr_start, pik3ca_info.chr_end, pik3ca_info.HGNC_ID,\"external_tools.json\", write_dir=\".cache\")\n",
    "    # sry_gnomad_variants.to_json(\".cache/gnomAD_test_HBB.json\")\n",
    "    print(\"test_queryGnomAD() passed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"test_queryGnomAD() failed. Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLINVAR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä BASIC STATISTICS:\n",
      "   Total variants: 593\n",
      "   Columns: 37\n",
      "   Chromosomes: ['11']\n",
      "\n",
      "üìã AVAILABLE COLUMNS (37):\n",
      "   - CHROM\n",
      "   - POS\n",
      "   - ID\n",
      "   - REF\n",
      "   - ALT\n",
      "   - QUAL\n",
      "   - FILTER\n",
      "   - AC\n",
      "   - AF\n",
      "   - AF_ESP\n",
      "   - AF_EXAC\n",
      "   - AF_TGP\n",
      "   - ALLELEID\n",
      "   - AN\n",
      "   - CLNDISDB\n",
      "   - CLNDISDBINCL\n",
      "   - CLNDN\n",
      "   - CLNDNINCL\n",
      "   - CLNHGVS\n",
      "   - CLNREVSTAT\n",
      "   - CLNSIG\n",
      "   - CLNSIGCONF\n",
      "   - CLNSIGINCL\n",
      "   - CLNVC\n",
      "   - CLNVCSO\n",
      "   - CLNVI\n",
      "   - DBVARID\n",
      "   - DP\n",
      "   - GENEINFO\n",
      "   - MC\n",
      "   - ORIGIN\n",
      "   - RS\n",
      "   - SSR\n",
      "   - is_pathogenic\n",
      "   - is_benign\n",
      "   - is_conflicting\n",
      "   - is_VUS\n",
      "\n",
      "üî¨ CLINICAL SIGNIFICANCE BREAKDOWN:\n",
      "   other: 272 (45.9%)\n",
      "   Pathogenic: 116 (19.6%)\n",
      "   Uncertain_significance: 75 (12.6%)\n",
      "   Pathogenic,_other: 42 (7.1%)\n",
      "   Likely_benign: 25 (4.2%)\n",
      "   Likely_pathogenic: 18 (3.0%)\n",
      "   Benign: 12 (2.0%)\n",
      "   Pathogenic/Likely_pathogenic: 10 (1.7%)\n",
      "   Conflicting_interpretations_of_pathogenicity: 9 (1.5%)\n",
      "   Conflicting_interpretations_of_pathogenicity,_other: 5 (0.8%)\n",
      "   Benign/Likely_benign: 1 (0.2%)\n",
      "   not_provided: 1 (0.2%)\n",
      "\n",
      "‚≠ê REVIEW STATUS BREAKDOWN:\n",
      "   no_assertion_criteria_provided: 369\n",
      "   criteria_provided,_single_submitter: 135\n",
      "   criteria_provided,_multiple_submitters,_no_conflicts: 67\n",
      "   criteria_provided,_conflicting_interpretations: 14\n",
      "   no_interpretation_for_the_single_variant: 7\n",
      "   no_assertion_provided: 1\n",
      "\n",
      "üö© PATHOGENICITY FLAGS:\n",
      "   is_pathogenic: 95 (16.0%)\n",
      "   is_benign: 33 (5.6%)\n",
      "   is_conflicting: 0 (0.0%)\n",
      "   is_VUS: 74 (12.5%)\n",
      "\n",
      "üß¨ GENE INFORMATION:\n",
      "   Unique genes: 4\n",
      "   Genes: ['HBB:3043|LOC107133510:107133510|LOC110006319:110006319', 'HBB:3043|LOC106099062:106099062|LOC107133510:107133510|LOC110006319:110006319', 'HBB:3043|LOC106099062:106099062|LOC107133510:107133510', 'HBB:3043|LOC106099062:106099062']\n",
      "\n",
      "üìç POSITION RANGE:\n",
      "   Min position: 5,225,466\n",
      "   Max position: 5,227,071\n",
      "   Span: 1,605 bp\n",
      "\n",
      "üìù SAMPLE VARIANTS (first 3):\n",
      "CHROM     POS REF ALT                 CLNSIG\n",
      "   11 5225466   G   A Uncertain_significance\n",
      "   11 5225481   G  GT Uncertain_significance\n",
      "   11 5225483   T   G Uncertain_significance\n",
      "\n",
      "================================================================================\n",
      "GNOMAD ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä BASIC STATISTICS:\n",
      "   Total variant-transcript pairs: 8946\n",
      "   Columns: 57\n",
      "   Chromosomes: ['11', None]\n",
      "   Unique variants: 720\n",
      "\n",
      "üìã AVAILABLE COLUMNS (57):\n",
      "   Core variant columns:\n",
      "      - CHROM\n",
      "      - POS\n",
      "      - ID\n",
      "      - REF\n",
      "      - ALT\n",
      "      - QUAL\n",
      "      - FILTER\n",
      "   Frequency columns:\n",
      "      - AC\n",
      "      - AF\n",
      "      - IMPACT\n",
      "      - DISTANCE\n",
      "      - STRAND\n",
      "      - VARIANT_CLASS\n",
      "      - CANONICAL\n",
      "      - MANE_SELECT\n",
      "      - MANE_PLUS_CLINICAL\n",
      "      - MOTIF_SCORE_CHANGE\n",
      "      ... and 1 more frequency columns\n",
      "   VEP annotation columns:\n",
      "      - vep\n",
      "      - index\n",
      "      - Allele\n",
      "      - Consequence\n",
      "      - SYMBOL\n",
      "      - Gene\n",
      "      - Feature_type\n",
      "      - Feature\n",
      "      - BIOTYPE\n",
      "      - EXON\n",
      "      - INTRON\n",
      "      - HGVSc\n",
      "      - HGVSp\n",
      "      - cDNA_position\n",
      "      - CDS_position\n",
      "      ... and 24 more VEP columns\n",
      "\n",
      "üìà ALLELE FREQUENCY STATISTICS:\n",
      "   Mean AF: 0.004083\n",
      "   Median AF: 0.000006\n",
      "   Min AF: 0.000001\n",
      "   Max AF: 0.818460\n",
      "\n",
      "   Frequency distribution:\n",
      "      Ultra-rare (AF < 0.00001): 4608 (51.5%)\n",
      "      Rare (0.00001 ‚â§ AF < 0.001): 1505 (16.8%)\n",
      "      Low frequency (0.001 ‚â§ AF < 0.01): 70 (0.8%)\n",
      "      Common (AF ‚â• 0.01): 56 (0.6%)\n",
      "\n",
      "üß¨ VARIANT CONSEQUENCES:\n",
      "   intron_variant: 1602 (17.9%)\n",
      "   downstream_gene_variant: 1538 (17.2%)\n",
      "   3_prime_UTR_variant: 636 (7.1%)\n",
      "   intron_variant&non_coding_transcript_variant: 534 (6.0%)\n",
      "   intron_variant&NMD_transcript_variant: 534 (6.0%)\n",
      "   missense_variant: 416 (4.7%)\n",
      "   synonymous_variant: 316 (3.5%)\n",
      "   3_prime_UTR_variant&NMD_transcript_variant: 316 (3.5%)\n",
      "   non_coding_transcript_exon_variant: 152 (1.7%)\n",
      "   splice_region_variant&splice_polypyrimidine_tract_variant&intron_variant: 21 (0.2%)\n",
      "\n",
      "‚ö° VARIANT IMPACT:\n",
      "   MODIFIER: 5312 (59.4%)\n",
      "   LOW: 442 (4.9%)\n",
      "   MODERATE: 428 (4.8%)\n",
      "   HIGH: 57 (0.6%)\n",
      "\n",
      "üß¨ GENE/TRANSCRIPT INFORMATION:\n",
      "   Unique genes: 1\n",
      "   Genes: ['HBB', None]\n",
      "   Unique transcripts: 7\n",
      "   HGNC IDs: ['HGNC:4827', None]\n",
      "\n",
      "üìù SAMPLE VARIANTS (first 3):\n",
      "CHROM     POS  REF  ALT       AF             Consequence   IMPACT HGVSp\n",
      "   11 5225464    T    C 0.000001     3_prime_UTR_variant MODIFIER      \n",
      "   11 5225464    T    C 0.000001 downstream_gene_variant MODIFIER      \n",
      " None    None None None      NaN                    None     None  None\n",
      "\n",
      "================================================================================\n",
      "DATASET COMPARISON\n",
      "================================================================================\n",
      "\n",
      "üîó VARIANT OVERLAP:\n",
      "   Total unique variants (union): 1193\n",
      "   In both datasets: 120 (10.1%)\n",
      "   ClinVar only: 473 (39.6%)\n",
      "   gnomAD only: 600 (50.3%)\n",
      "\n",
      "üéØ OVERLAPPING VARIANTS ANALYSIS:\n",
      "\n",
      "   Clinical significance in overlapping variants:\n",
      "      Uncertain_significance: 35\n",
      "      other: 27\n",
      "      Pathogenic: 22\n",
      "      Likely_benign: 15\n",
      "      Benign: 8\n",
      "      Likely_pathogenic: 4\n",
      "      Conflicting_interpretations_of_pathogenicity: 4\n",
      "      Pathogenic,_other: 2\n",
      "      Conflicting_interpretations_of_pathogenicity,_other: 2\n",
      "      Benign/Likely_benign: 1\n",
      "\n",
      "================================================================================\n",
      "ACTIONABLE INSIGHTS & RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "üí° WHAT YOU CAN DO WITH THIS DATA:\n",
      "\n",
      "1Ô∏è‚É£  CLINVAR DATA APPLICATIONS:\n",
      "   ‚úì Identify 95 high-confidence pathogenic variants\n",
      "   ‚úì Use for genetic testing interpretation\n",
      "   ‚úì Prioritize variants for functional validation\n",
      "   ‚úì Found 74 Variants of Uncertain Significance (VUS)\n",
      "   ‚úì Candidates for reclassification research\n",
      "\n",
      "2Ô∏è‚É£  GNOMAD DATA APPLICATIONS:\n",
      "   ‚úì Filter 6113 rare variants (AF < 0.1%)\n",
      "   ‚úì Identify novel/ultra-rare variants for research\n",
      "   ‚úì Population-specific allele frequency analysis\n",
      "   ‚úì Analyze 428 missense variants for pathogenicity prediction\n",
      "\n",
      "3Ô∏è‚É£  INTEGRATED ANALYSIS:\n",
      "   ‚úì Cross-validate 120 variants with both clinical and population data\n",
      "   ‚úì Identify potential ClinVar reclassification candidates\n",
      "   ‚úì Build pathogenicity prediction models\n",
      "\n",
      "4Ô∏è‚É£  RECOMMENDED ANALYSES:\n",
      "   üìä Variant prioritization pipeline:\n",
      "      1. Filter gnomAD for rare variants (AF < 0.001)\n",
      "      2. Annotate with ClinVar pathogenicity\n",
      "      3. Prioritize HIGH/MODERATE impact variants\n",
      "      4. Focus on canonical transcripts\n",
      "\n",
      "   üî¨ Research applications:\n",
      "      ‚Ä¢ Gene burden analysis\n",
      "      ‚Ä¢ Rare variant association studies\n",
      "      ‚Ä¢ Pathogenicity prediction training\n",
      "      ‚Ä¢ Clinical variant reclassification\n",
      "\n",
      "   üß¨ Clinical applications:\n",
      "      ‚Ä¢ Patient variant interpretation\n",
      "      ‚Ä¢ Genetic testing result validation\n",
      "      ‚Ä¢ Carrier screening design\n",
      "      ‚Ä¢ Pharmacogenomic analysis\n",
      "\n",
      "================================================================================\n",
      "‚úÖ ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive analysis script for ClinVar and gnomAD JSON outputs\n",
    "\n",
    "Usage:\n",
    "    python analyze_variant_data.py --clinvar path/to/clinvar.json --gnomad path/to/gnomad.json\n",
    "\n",
    "Or use as a module:\n",
    "    from analyze_variant_data import analyze_clinvar, analyze_gnomad, compare_datasets\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from typing import Dict, Tuple, Optional\n",
    "import sys\n",
    "\n",
    "def load_json_data(filepath: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"Load JSON file and convert to DataFrame\"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Handle different JSON formats\n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    elif isinstance(data, dict):\n",
    "        # Check if this is a dict of arrays (your case)\n",
    "        first_val = next(iter(data.values()))\n",
    "        if isinstance(first_val, dict):\n",
    "            # This is the problematic format - need to reconstruct\n",
    "            # Find the maximum length across all columns\n",
    "            max_length = max(len(v) for v in data.values())\n",
    "            reconstructed = []\n",
    "            for i in range(max_length):\n",
    "                row = {}\n",
    "                for col in data.keys():\n",
    "                    # Use .get() with None default for missing indices\n",
    "                    row[col] = data[col].get(str(i), None)\n",
    "                reconstructed.append(row)\n",
    "            df = pd.DataFrame(reconstructed)\n",
    "        else:\n",
    "            df = pd.DataFrame([data])\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected JSON format in {filepath}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_clinvar(clinvar_json_path: str | Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of ClinVar data\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing analysis results and the DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLINVAR ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df = load_json_data(clinvar_json_path)\n",
    "    \n",
    "    analysis = {\n",
    "        'dataframe': df,\n",
    "        'total_variants': len(df),\n",
    "        'columns': list(df.columns),\n",
    "        'shape': df.shape\n",
    "    }\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä BASIC STATISTICS:\")\n",
    "    print(f\"   Total variants: {len(df)}\")\n",
    "    print(f\"   Columns: {df.shape[1]}\")\n",
    "    \n",
    "    # Safe chromosome extraction\n",
    "    if 'CHROM' in df.columns:\n",
    "        try:\n",
    "            # Try to get unique values, handling cases where CHROM might be dict or other types\n",
    "            chrom_values = df['CHROM'].apply(lambda x: str(x) if not isinstance(x, (dict, list)) else str(x)).unique().tolist()\n",
    "            print(f\"   Chromosomes: {chrom_values}\")\n",
    "        except:\n",
    "            print(f\"   Chromosomes: [complex data type]\")\n",
    "    \n",
    "    # Column summary\n",
    "    print(f\"\\nüìã AVAILABLE COLUMNS ({len(df.columns)}):\")\n",
    "    for col in df.columns:\n",
    "        print(f\"   - {col}\")\n",
    "    \n",
    "    # Clinical significance analysis\n",
    "    if 'CLNSIG' in df.columns:\n",
    "        print(f\"\\nüî¨ CLINICAL SIGNIFICANCE BREAKDOWN:\")\n",
    "        clnsig_counts = df['CLNSIG'].value_counts()\n",
    "        analysis['clinical_significance'] = clnsig_counts.to_dict()\n",
    "        for sig, count in clnsig_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   {sig}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Review status analysis\n",
    "    if 'CLNREVSTAT' in df.columns:\n",
    "        print(f\"\\n‚≠ê REVIEW STATUS BREAKDOWN:\")\n",
    "        revstat_counts = df['CLNREVSTAT'].value_counts()\n",
    "        analysis['review_status'] = revstat_counts.to_dict()\n",
    "        for status, count in revstat_counts.items():\n",
    "            print(f\"   {status}: {count}\")\n",
    "    \n",
    "    # Pathogenicity flags (if present)\n",
    "    pathogenicity_flags = ['is_pathogenic', 'is_benign', 'is_conflicting', 'is_VUS']\n",
    "    if any(flag in df.columns for flag in pathogenicity_flags):\n",
    "        print(f\"\\nüö© PATHOGENICITY FLAGS:\")\n",
    "        for flag in pathogenicity_flags:\n",
    "            if flag in df.columns:\n",
    "                count = df[flag].sum() if df[flag].dtype == bool else df[flag].astype(bool).sum()\n",
    "                percentage = (count / len(df)) * 100\n",
    "                analysis[flag] = int(count)\n",
    "                print(f\"   {flag}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Gene information\n",
    "    if 'GENEINFO' in df.columns:\n",
    "        unique_genes = df['GENEINFO'].nunique()\n",
    "        print(f\"\\nüß¨ GENE INFORMATION:\")\n",
    "        print(f\"   Unique genes: {unique_genes}\")\n",
    "        if unique_genes <= 10:\n",
    "            print(f\"   Genes: {df['GENEINFO'].unique().tolist()}\")\n",
    "    \n",
    "    if 'POS' in df.columns:\n",
    "        print(f\"\\nüìç POSITION RANGE:\")\n",
    "        pos_min = int(df['POS'].min())\n",
    "        pos_max = int(df['POS'].max())\n",
    "        print(f\"   Min position: {pos_min:,}\")\n",
    "        print(f\"   Max position: {pos_max:,}\")\n",
    "        print(f\"   Span: {pos_max - pos_min:,} bp\")\n",
    "        analysis['position_range'] = {\n",
    "            'min': pos_min,\n",
    "            'max': pos_max,\n",
    "            'span': pos_max - pos_min\n",
    "        }\n",
    "    \n",
    "    # Sample variants\n",
    "    print(f\"\\nüìù SAMPLE VARIANTS (first 3):\")\n",
    "    sample_cols = ['CHROM', 'POS', 'REF', 'ALT', 'CLNSIG']\n",
    "    available_cols = [col for col in sample_cols if col in df.columns]\n",
    "    if available_cols:\n",
    "        print(df[available_cols].head(3).to_string(index=False))\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def analyze_gnomad(gnomad_json_path: str | Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of gnomAD data\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing analysis results and the DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GNOMAD ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df = load_json_data(gnomad_json_path)\n",
    "    \n",
    "    analysis = {\n",
    "        'dataframe': df,\n",
    "        'total_variants': len(df),\n",
    "        'columns': list(df.columns),\n",
    "        'shape': df.shape\n",
    "    }\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä BASIC STATISTICS:\")\n",
    "    print(f\"   Total variant-transcript pairs: {len(df)}\")\n",
    "    print(f\"   Columns: {df.shape[1]}\")\n",
    "    print(f\"   Chromosomes: {df['CHROM'].unique().tolist() if 'CHROM' in df.columns else 'N/A'}\")\n",
    "    \n",
    "    # Unique variants (before transcript expansion)\n",
    "    if all(col in df.columns for col in ['CHROM', 'POS', 'REF', 'ALT']):\n",
    "        unique_variants = df[['CHROM', 'POS', 'REF', 'ALT']].drop_duplicates()\n",
    "        print(f\"   Unique variants: {len(unique_variants)}\")\n",
    "        analysis['unique_variants'] = len(unique_variants)\n",
    "    \n",
    "    # Column summary\n",
    "    print(f\"\\nüìã AVAILABLE COLUMNS ({len(df.columns)}):\")\n",
    "    print(\"   Core variant columns:\")\n",
    "    core_cols = ['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER']\n",
    "    for col in core_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"      - {col}\")\n",
    "    \n",
    "    print(\"   Frequency columns:\")\n",
    "    freq_cols = [col for col in df.columns if 'AF' in col or 'AC' in col or 'AN' in col]\n",
    "    for col in freq_cols[:10]:  # Show first 10\n",
    "        print(f\"      - {col}\")\n",
    "    if len(freq_cols) > 10:\n",
    "        print(f\"      ... and {len(freq_cols) - 10} more frequency columns\")\n",
    "    \n",
    "    print(\"   VEP annotation columns:\")\n",
    "    vep_cols = [col for col in df.columns if col not in core_cols + freq_cols]\n",
    "    for col in vep_cols[:15]:  # Show first 15\n",
    "        print(f\"      - {col}\")\n",
    "    if len(vep_cols) > 15:\n",
    "        print(f\"      ... and {len(vep_cols) - 15} more VEP columns\")\n",
    "    \n",
    "    # Allele frequency analysis\n",
    "    if 'AF' in df.columns:\n",
    "        print(f\"\\nüìà ALLELE FREQUENCY STATISTICS:\")\n",
    "        af_series = pd.to_numeric(df['AF'], errors='coerce')\n",
    "        analysis['allele_frequency'] = {\n",
    "            'mean': float(af_series.mean()),\n",
    "            'median': float(af_series.median()),\n",
    "            'min': float(af_series.min()),\n",
    "            'max': float(af_series.max())\n",
    "        }\n",
    "        print(f\"   Mean AF: {af_series.mean():.6f}\")\n",
    "        print(f\"   Median AF: {af_series.median():.6f}\")\n",
    "        print(f\"   Min AF: {af_series.min():.6f}\")\n",
    "        print(f\"   Max AF: {af_series.max():.6f}\")\n",
    "        \n",
    "        # Frequency bins\n",
    "        print(f\"\\n   Frequency distribution:\")\n",
    "        ultra_rare = (af_series < 0.00001).sum()\n",
    "        rare = ((af_series >= 0.00001) & (af_series < 0.001)).sum()\n",
    "        low_freq = ((af_series >= 0.001) & (af_series < 0.01)).sum()\n",
    "        common = (af_series >= 0.01).sum()\n",
    "        print(f\"      Ultra-rare (AF < 0.00001): {ultra_rare} ({ultra_rare/len(df)*100:.1f}%)\")\n",
    "        print(f\"      Rare (0.00001 ‚â§ AF < 0.001): {rare} ({rare/len(df)*100:.1f}%)\")\n",
    "        print(f\"      Low frequency (0.001 ‚â§ AF < 0.01): {low_freq} ({low_freq/len(df)*100:.1f}%)\")\n",
    "        print(f\"      Common (AF ‚â• 0.01): {common} ({common/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Consequence analysis (VEP)\n",
    "    if 'Consequence' in df.columns:\n",
    "        print(f\"\\nüß¨ VARIANT CONSEQUENCES:\")\n",
    "        consequence_counts = df['Consequence'].value_counts().head(10)\n",
    "        analysis['consequences'] = consequence_counts.to_dict()\n",
    "        for consequence, count in consequence_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   {consequence}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Impact analysis\n",
    "    if 'IMPACT' in df.columns:\n",
    "        print(f\"\\n‚ö° VARIANT IMPACT:\")\n",
    "        impact_counts = df['IMPACT'].value_counts()\n",
    "        analysis['impact'] = impact_counts.to_dict()\n",
    "        for impact, count in impact_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   {impact}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Gene/transcript information\n",
    "    if 'SYMBOL' in df.columns:\n",
    "        unique_genes = df['SYMBOL'].nunique()\n",
    "        print(f\"\\nüß¨ GENE/TRANSCRIPT INFORMATION:\")\n",
    "        print(f\"   Unique genes: {unique_genes}\")\n",
    "        if unique_genes <= 5:\n",
    "            print(f\"   Genes: {df['SYMBOL'].unique().tolist()}\")\n",
    "    \n",
    "    if 'Feature' in df.columns:\n",
    "        print(f\"   Unique transcripts: {df['Feature'].nunique()}\")\n",
    "    \n",
    "    # HGNC ID\n",
    "    if 'HGNC_ID' in df.columns:\n",
    "        print(f\"   HGNC IDs: {df['HGNC_ID'].unique().tolist()}\")\n",
    "    \n",
    "    # Sample variants\n",
    "    print(f\"\\nüìù SAMPLE VARIANTS (first 3):\")\n",
    "    sample_cols = ['CHROM', 'POS', 'REF', 'ALT', 'AF', 'Consequence', 'IMPACT', 'HGVSp']\n",
    "    available_cols = [col for col in sample_cols if col in df.columns]\n",
    "    if available_cols:\n",
    "        print(df[available_cols].head(3).to_string(index=False))\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def compare_datasets(clinvar_df: pd.DataFrame, gnomad_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare ClinVar and gnomAD datasets to find overlapping variants\n",
    "    \n",
    "    Returns:\n",
    "        Dict with comparison statistics and merged data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Ensure coordinate columns are strings for matching\n",
    "    for df in [clinvar_df, gnomad_df]:\n",
    "        for col in ['CHROM', 'POS', 'REF', 'ALT']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Merge on variant coordinates\n",
    "    merge_cols = ['CHROM', 'POS', 'REF', 'ALT']\n",
    "    if all(col in clinvar_df.columns and col in gnomad_df.columns for col in merge_cols):\n",
    "        \n",
    "        # Get unique variants from gnomAD (it has transcript duplicates)\n",
    "        gnomad_unique = gnomad_df[merge_cols + ['AF']].drop_duplicates(subset=merge_cols)\n",
    "        \n",
    "        merged = pd.merge(\n",
    "            clinvar_df,\n",
    "            gnomad_unique,\n",
    "            on=merge_cols,\n",
    "            how='outer',\n",
    "            indicator=True,\n",
    "            suffixes=('_clinvar', '_gnomad')\n",
    "        )\n",
    "        \n",
    "        both = merged[merged['_merge'] == 'both']\n",
    "        clinvar_only = merged[merged['_merge'] == 'left_only']\n",
    "        gnomad_only = merged[merged['_merge'] == 'right_only']\n",
    "        \n",
    "        print(f\"\\nüîó VARIANT OVERLAP:\")\n",
    "        print(f\"   Total unique variants (union): {len(merged)}\")\n",
    "        print(f\"   In both datasets: {len(both)} ({len(both)/len(merged)*100:.1f}%)\")\n",
    "        print(f\"   ClinVar only: {len(clinvar_only)} ({len(clinvar_only)/len(merged)*100:.1f}%)\")\n",
    "        print(f\"   gnomAD only: {len(gnomad_only)} ({len(gnomad_only)/len(merged)*100:.1f}%)\")\n",
    "        \n",
    "        # Analyze overlapping variants\n",
    "        if len(both) > 0:\n",
    "            print(f\"\\nüéØ OVERLAPPING VARIANTS ANALYSIS:\")\n",
    "            \n",
    "            # Clinical significance of overlapping variants\n",
    "            if 'CLNSIG' in both.columns:\n",
    "                print(f\"\\n   Clinical significance in overlapping variants:\")\n",
    "                for sig, count in both['CLNSIG'].value_counts().items():\n",
    "                    print(f\"      {sig}: {count}\")\n",
    "            \n",
    "            # Allele frequency of pathogenic variants\n",
    "            if 'is_pathogenic' in both.columns and 'AF' in both.columns:\n",
    "                pathogenic = both[both['is_pathogenic'] == True]\n",
    "                if len(pathogenic) > 0:\n",
    "                    af_series = pd.to_numeric(pathogenic['AF'], errors='coerce')\n",
    "                    print(f\"\\n   Pathogenic variants in gnomAD:\")\n",
    "                    print(f\"      Count: {len(pathogenic)}\")\n",
    "                    print(f\"      Mean AF: {af_series.mean():.6f}\")\n",
    "                    print(f\"      Median AF: {af_series.median():.6f}\")\n",
    "                    \n",
    "                    # Flag concerning variants (pathogenic but common)\n",
    "                    concerning = pathogenic[af_series > 0.01]\n",
    "                    if len(concerning) > 0:\n",
    "                        print(f\"\\n   ‚ö†Ô∏è  WARNING: {len(concerning)} pathogenic variants with AF > 1%\")\n",
    "                        print(f\"      (These may be misclassified or benign)\")\n",
    "        \n",
    "        comparison = {\n",
    "            'merged_df': merged,\n",
    "            'total_unique': len(merged),\n",
    "            'in_both': len(both),\n",
    "            'clinvar_only': len(clinvar_only),\n",
    "            'gnomad_only': len(gnomad_only),\n",
    "            'both_df': both\n",
    "        }\n",
    "        \n",
    "        return comparison\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Cannot compare: Missing coordinate columns in one or both datasets\")\n",
    "        return {'error': 'Missing coordinate columns'}\n",
    "\n",
    "\n",
    "def generate_actionable_insights(clinvar_analysis: Dict, gnomad_analysis: Dict, \n",
    "                                 comparison: Optional[Dict] = None):\n",
    "    \"\"\"\n",
    "    Generate actionable insights and recommendations based on the data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ACTIONABLE INSIGHTS & RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    clinvar_df = clinvar_analysis['dataframe']\n",
    "    gnomad_df = gnomad_analysis['dataframe']\n",
    "    \n",
    "    print(f\"\\nüí° WHAT YOU CAN DO WITH THIS DATA:\\n\")\n",
    "    \n",
    "    # ClinVar insights\n",
    "    print(\"1Ô∏è‚É£  CLINVAR DATA APPLICATIONS:\")\n",
    "    if 'is_pathogenic' in clinvar_df.columns:\n",
    "        pathogenic_count = clinvar_df['is_pathogenic'].sum()\n",
    "        print(f\"   ‚úì Identify {pathogenic_count} high-confidence pathogenic variants\")\n",
    "        print(f\"   ‚úì Use for genetic testing interpretation\")\n",
    "        print(f\"   ‚úì Prioritize variants for functional validation\")\n",
    "    \n",
    "    if 'is_VUS' in clinvar_df.columns:\n",
    "        vus_count = clinvar_df['is_VUS'].sum()\n",
    "        print(f\"   ‚úì Found {vus_count} Variants of Uncertain Significance (VUS)\")\n",
    "        print(f\"   ‚úì Candidates for reclassification research\")\n",
    "    \n",
    "    # gnomAD insights\n",
    "    print(f\"\\n2Ô∏è‚É£  GNOMAD DATA APPLICATIONS:\")\n",
    "    if 'AF' in gnomad_df.columns:\n",
    "        af_series = pd.to_numeric(gnomad_df['AF'], errors='coerce')\n",
    "        rare_count = (af_series < 0.001).sum()\n",
    "        print(f\"   ‚úì Filter {rare_count} rare variants (AF < 0.1%)\")\n",
    "        print(f\"   ‚úì Identify novel/ultra-rare variants for research\")\n",
    "        print(f\"   ‚úì Population-specific allele frequency analysis\")\n",
    "    \n",
    "    if 'Consequence' in gnomad_df.columns:\n",
    "        missense = (gnomad_df['Consequence'].str.contains('missense', case=False, na=False)).sum()\n",
    "        print(f\"   ‚úì Analyze {missense} missense variants for pathogenicity prediction\")\n",
    "    \n",
    "    # Combined insights\n",
    "    if comparison and 'both_df' in comparison:\n",
    "        both_df = comparison['both_df']\n",
    "        print(f\"\\n3Ô∏è‚É£  INTEGRATED ANALYSIS:\")\n",
    "        print(f\"   ‚úì Cross-validate {len(both_df)} variants with both clinical and population data\")\n",
    "        \n",
    "        if 'is_pathogenic' in both_df.columns and 'AF' in both_df.columns:\n",
    "            pathogenic = both_df[both_df['is_pathogenic'] == True]\n",
    "            af_series = pd.to_numeric(pathogenic['AF'], errors='coerce')\n",
    "            rare_pathogenic = (af_series < 0.001).sum()\n",
    "            print(f\"   ‚úì {rare_pathogenic} rare pathogenic variants (strong disease candidates)\")\n",
    "        \n",
    "        print(f\"   ‚úì Identify potential ClinVar reclassification candidates\")\n",
    "        print(f\"   ‚úì Build pathogenicity prediction models\")\n",
    "    \n",
    "    # Specific use cases\n",
    "    print(f\"\\n4Ô∏è‚É£  RECOMMENDED ANALYSES:\")\n",
    "    print(f\"   üìä Variant prioritization pipeline:\")\n",
    "    print(f\"      1. Filter gnomAD for rare variants (AF < 0.001)\")\n",
    "    print(f\"      2. Annotate with ClinVar pathogenicity\")\n",
    "    print(f\"      3. Prioritize HIGH/MODERATE impact variants\")\n",
    "    print(f\"      4. Focus on canonical transcripts\")\n",
    "    \n",
    "    print(f\"\\n   üî¨ Research applications:\")\n",
    "    print(f\"      ‚Ä¢ Gene burden analysis\")\n",
    "    print(f\"      ‚Ä¢ Rare variant association studies\")\n",
    "    print(f\"      ‚Ä¢ Pathogenicity prediction training\")\n",
    "    print(f\"      ‚Ä¢ Clinical variant reclassification\")\n",
    "    \n",
    "    print(f\"\\n   üß¨ Clinical applications:\")\n",
    "    print(f\"      ‚Ä¢ Patient variant interpretation\")\n",
    "    print(f\"      ‚Ä¢ Genetic testing result validation\")\n",
    "    print(f\"      ‚Ä¢ Carrier screening design\")\n",
    "    print(f\"      ‚Ä¢ Pharmacogenomic analysis\")\n",
    "\n",
    "\n",
    "def save_analysis_report(output_path: str | Path, clinvar_analysis: Dict, \n",
    "                        gnomad_analysis: Dict, comparison: Optional[Dict] = None):\n",
    "    \"\"\"Save analysis results to a text file\"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(\"VARIANT DATA ANALYSIS REPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"CLINVAR SUMMARY\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Total variants: {clinvar_analysis['total_variants']}\\n\")\n",
    "        f.write(f\"Columns: {clinvar_analysis['shape'][1]}\\n\")\n",
    "        if 'clinical_significance' in clinvar_analysis:\n",
    "            f.write(\"\\nClinical Significance:\\n\")\n",
    "            for sig, count in clinvar_analysis['clinical_significance'].items():\n",
    "                f.write(f\"  {sig}: {count}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\\nGNOMAD SUMMARY\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Total variant-transcript pairs: {gnomad_analysis['total_variants']}\\n\")\n",
    "        f.write(f\"Columns: {gnomad_analysis['shape'][1]}\\n\")\n",
    "        if 'unique_variants' in gnomad_analysis:\n",
    "            f.write(f\"Unique variants: {gnomad_analysis['unique_variants']}\\n\")\n",
    "        \n",
    "        if comparison and 'total_unique' in comparison:\n",
    "            f.write(\"\\n\\nCOMPARISON SUMMARY\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            f.write(f\"Total unique variants: {comparison['total_unique']}\\n\")\n",
    "            f.write(f\"In both datasets: {comparison['in_both']}\\n\")\n",
    "            f.write(f\"ClinVar only: {comparison['clinvar_only']}\\n\")\n",
    "            f.write(f\"gnomAD only: {comparison['gnomad_only']}\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis report saved to: {output_path}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ===== SET YOUR FILE PATHS HERE =====\n",
    "    CLINVAR_JSON = \".cache/HBB_clinvar.json\"  # Set to None if you don't want to analyze ClinVar\n",
    "    GNOMAD_JSON = \".cache/gnomAD_test_HBB.json\"      # Set to None if you don't want to analyze gnomAD\n",
    "    OUTPUT_REPORT = None                          # Set to a path like \"report.txt\" to save report, or None to skip\n",
    "    \n",
    "    if not CLINVAR_JSON and not GNOMAD_JSON:\n",
    "        raise ValueError(\"At least one of CLINVAR_JSON or GNOMAD_JSON must be specified\")\n",
    "    \n",
    "    clinvar_analysis = None\n",
    "    gnomad_analysis = None\n",
    "    comparison = None\n",
    "    \n",
    "    try:\n",
    "        # Analyze ClinVar\n",
    "        if CLINVAR_JSON:\n",
    "            clinvar_analysis = analyze_clinvar(CLINVAR_JSON)\n",
    "        \n",
    "        # Analyze gnomAD\n",
    "        if GNOMAD_JSON:\n",
    "            gnomad_analysis = analyze_gnomad(GNOMAD_JSON)\n",
    "        \n",
    "        # Compare if both provided\n",
    "        if clinvar_analysis and gnomad_analysis:\n",
    "            comparison = compare_datasets(\n",
    "                clinvar_analysis['dataframe'],\n",
    "                gnomad_analysis['dataframe']\n",
    "            )\n",
    "        \n",
    "        # Generate insights\n",
    "        if clinvar_analysis and gnomad_analysis:\n",
    "            generate_actionable_insights(clinvar_analysis, gnomad_analysis, comparison)\n",
    "        \n",
    "        # Save report if requested\n",
    "        if OUTPUT_REPORT:\n",
    "            save_analysis_report(OUTPUT_REPORT, clinvar_analysis or {}, \n",
    "                               gnomad_analysis or {}, comparison)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: {e}\", file=sys.stderr)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MISSENSE VARIANT OVERLAP ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä ClinVar missense variants: 383\n",
      "üìä gnomAD missense variants: 203\n",
      "\n",
      "üîó MISSENSE VARIANT OVERLAP:\n",
      "   Total unique missense variants: 481\n",
      "   In both datasets: 105 (21.8%)\n",
      "   ClinVar only: 278 (57.8%)\n",
      "   gnomAD only: 98 (20.4%)\n",
      "\n",
      "üéØ OVERLAPPING MISSENSE VARIANTS:\n",
      "\n",
      "   Clinical significance:\n",
      "      other: 50\n",
      "      Uncertain_significance: 17\n",
      "      Pathogenic: 12\n",
      "      Likely_benign: 9\n",
      "      Conflicting_interpretations_of_pathogenicity,_other: 5\n",
      "      Pathogenic,_other: 4\n",
      "      Benign: 3\n",
      "      Likely_pathogenic: 3\n",
      "      Conflicting_interpretations_of_pathogenicity: 2\n",
      "\n",
      "   Pathogenic missense variants:\n",
      "      Count: 11\n",
      "      Mean AF: 0.001543\n",
      "      Median AF: 0.000020\n",
      "      Range: 0.000001 - 0.012719\n",
      "\n",
      "   Example pathogenic missense variants:\n",
      "    POS REF ALT            CLNSIG  AF_gnomad                         HGVSp\n",
      "5225662   A   C        Pathogenic   0.000002 ENSP00000333994.3:p.Val127Gly\n",
      "5225678   C   T        Pathogenic   0.000033 ENSP00000333994.3:p.Glu122Lys\n",
      "5225710   A   G        Pathogenic   0.000013 ENSP00000333994.3:p.Leu111Pro\n",
      "5225714   C   T Likely_pathogenic   0.000001 ENSP00000333994.3:p.Val110Met\n",
      "5226597   C   T        Pathogenic   0.000001  ENSP00000333994.3:p.Val99Met\n",
      "5226930   C   G        Pathogenic   0.000020  ENSP00000333994.3:p.Arg31Thr\n",
      "5226930   C   T Likely_pathogenic   0.000005  ENSP00000333994.3:p.Arg31Lys\n",
      "5226940   C   A Likely_pathogenic   0.000020  ENSP00000333994.3:p.Ala28Ser\n",
      "5226943   C   T        Pathogenic   0.000414  ENSP00000333994.3:p.Glu27Lys\n",
      "5227002   T   A        Pathogenic   0.012719   ENSP00000333994.3:p.Glu7Val\n",
      "\n",
      "üìã CLINVAR-ONLY MISSENSE (no population data):\n",
      "   Count: 278\n",
      "\n",
      "   Clinical significance breakdown:\n",
      "      other: 200\n",
      "      Pathogenic,_other: 35\n",
      "      Pathogenic: 21\n",
      "      Uncertain_significance: 10\n",
      "      Benign: 2\n",
      "\n",
      "   Interpretation:\n",
      "      ‚Ä¢ Ultra-rare private mutations\n",
      "      ‚Ä¢ Not seen in gnomAD populations\n",
      "      ‚Ä¢ Potentially novel disease variants\n",
      "\n",
      "üìã GNOMAD-ONLY MISSENSE (no clinical annotation):\n",
      "   Count: 98\n",
      "   Mean AF: 0.008356\n",
      "   Median AF: 0.000001\n",
      "   Rare variants (AF < 0.1%): 97\n",
      "   Common variants (AF ‚â• 1%): 1\n",
      "\n",
      "   Interpretation:\n",
      "      ‚Ä¢ Likely benign population variants\n",
      "      ‚Ä¢ VUS candidates needing clinical interpretation\n",
      "      ‚Ä¢ Common polymorphisms\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SUMMARY:\n",
      "‚úì ClinVar missense: 383\n",
      "‚úì gnomAD missense: 203\n",
      "‚úì Overlap: 105 (21.8%)\n",
      "‚úì ClinVar pathogenic in overlap: 11\n",
      "‚úì Gold standard training pairs: 105\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_99819/4001673238.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['CHROM'] = df['CHROM'].astype(str)\n",
      "/var/tmp/ipykernel_99819/4001673238.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['POS'] = df['POS'].astype(str)\n",
      "/var/tmp/ipykernel_99819/4001673238.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['REF'] = df['REF'].astype(str)\n",
      "/var/tmp/ipykernel_99819/4001673238.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['ALT'] = df['ALT'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load both datasets\n",
    "with open('.cache/HBB_clinvar.json', 'r') as f:\n",
    "    clinvar_data = json.load(f)\n",
    "clinvar_df = pd.DataFrame(clinvar_data)\n",
    "\n",
    "with open('.cache/gnomAD_test_HBB.json', 'r') as f:\n",
    "    gnomad_data = json.load(f)\n",
    "gnomad_df = pd.DataFrame(gnomad_data)\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MISSENSE VARIANT OVERLAP ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter ClinVar for missense variants\n",
    "clinvar_missense = clinvar_df[\n",
    "    clinvar_df['CLNVC'].str.contains('missense', case=False, na=False) |\n",
    "    clinvar_df['MC'].str.contains('missense', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nüìä ClinVar missense variants: {len(clinvar_missense)}\")\n",
    "\n",
    "# Filter gnomAD for missense variants\n",
    "gnomad_missense = gnomad_df[\n",
    "    gnomad_df['Consequence'].str.contains('missense', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "# Get unique gnomAD missense (remove transcript duplicates)\n",
    "gnomad_missense_unique = gnomad_missense.drop_duplicates(\n",
    "    subset=['CHROM', 'POS', 'REF', 'ALT']\n",
    ")\n",
    "\n",
    "print(f\"üìä gnomAD missense variants: {len(gnomad_missense_unique)}\")\n",
    "\n",
    "# Standardize coordinates for merging\n",
    "for df in [clinvar_missense, gnomad_missense_unique]:\n",
    "    df['CHROM'] = df['CHROM'].astype(str)\n",
    "    df['POS'] = df['POS'].astype(str)\n",
    "    df['REF'] = df['REF'].astype(str)\n",
    "    df['ALT'] = df['ALT'].astype(str)\n",
    "\n",
    "# Merge to find overlap\n",
    "merged_missense = pd.merge(\n",
    "    clinvar_missense,\n",
    "    gnomad_missense_unique,\n",
    "    on=['CHROM', 'POS', 'REF', 'ALT'],\n",
    "    how='outer',\n",
    "    indicator=True,\n",
    "    suffixes=('_clinvar', '_gnomad')\n",
    ")\n",
    "\n",
    "both = merged_missense[merged_missense['_merge'] == 'both']\n",
    "clinvar_only = merged_missense[merged_missense['_merge'] == 'left_only']\n",
    "gnomad_only = merged_missense[merged_missense['_merge'] == 'right_only']\n",
    "\n",
    "print(f\"\\nüîó MISSENSE VARIANT OVERLAP:\")\n",
    "print(f\"   Total unique missense variants: {len(merged_missense)}\")\n",
    "print(f\"   In both datasets: {len(both)} ({len(both)/len(merged_missense)*100:.1f}%)\")\n",
    "print(f\"   ClinVar only: {len(clinvar_only)} ({len(clinvar_only)/len(merged_missense)*100:.1f}%)\")\n",
    "print(f\"   gnomAD only: {len(gnomad_only)} ({len(gnomad_only)/len(merged_missense)*100:.1f}%)\")\n",
    "\n",
    "# Analyze overlapping missense variants\n",
    "if len(both) > 0:\n",
    "    print(f\"\\nüéØ OVERLAPPING MISSENSE VARIANTS:\")\n",
    "    \n",
    "    # Clinical significance distribution\n",
    "    if 'CLNSIG' in both.columns:\n",
    "        print(f\"\\n   Clinical significance:\")\n",
    "        clnsig_counts = both['CLNSIG'].value_counts().head(10)\n",
    "        for sig, count in clnsig_counts.items():\n",
    "            print(f\"      {sig}: {count}\")\n",
    "    \n",
    "    # Check which AF column exists\n",
    "    af_col = 'AF_gnomad' if 'AF_gnomad' in both.columns else 'AF' if 'AF' in both.columns else None\n",
    "    \n",
    "    # Allele frequencies of pathogenic missense\n",
    "    if 'is_pathogenic' in both.columns and af_col:\n",
    "        pathogenic_missense = both[both['is_pathogenic'] == True]\n",
    "        if len(pathogenic_missense) > 0:\n",
    "            af_series = pd.to_numeric(pathogenic_missense[af_col], errors='coerce').dropna()\n",
    "            if len(af_series) > 0:\n",
    "                print(f\"\\n   Pathogenic missense variants:\")\n",
    "                print(f\"      Count: {len(pathogenic_missense)}\")\n",
    "                print(f\"      Mean AF: {af_series.mean():.6f}\")\n",
    "                print(f\"      Median AF: {af_series.median():.6f}\")\n",
    "                print(f\"      Range: {af_series.min():.6f} - {af_series.max():.6f}\")\n",
    "            \n",
    "            # Show examples with HGVSp\n",
    "            print(f\"\\n   Example pathogenic missense variants:\")\n",
    "            example_cols = ['POS', 'REF', 'ALT', 'CLNSIG']\n",
    "            if af_col:\n",
    "                example_cols.append(af_col)\n",
    "            if 'HGVSp_gnomad' in pathogenic_missense.columns:\n",
    "                example_cols.append('HGVSp_gnomad')\n",
    "            elif 'HGVSp' in pathogenic_missense.columns:\n",
    "                example_cols.append('HGVSp')\n",
    "            \n",
    "            available_cols = [c for c in example_cols if c in pathogenic_missense.columns]\n",
    "            if available_cols:\n",
    "                print(pathogenic_missense[available_cols].head(10).to_string(index=False))\n",
    "\n",
    "# Show ClinVar-only missense\n",
    "print(f\"\\nüìã CLINVAR-ONLY MISSENSE (no population data):\")\n",
    "print(f\"   Count: {len(clinvar_only)}\")\n",
    "if len(clinvar_only) > 0 and 'CLNSIG' in clinvar_only.columns:\n",
    "    print(f\"\\n   Clinical significance breakdown:\")\n",
    "    clnsig_only = clinvar_only['CLNSIG'].value_counts().head(5)\n",
    "    for sig, count in clnsig_only.items():\n",
    "        print(f\"      {sig}: {count}\")\n",
    "    print(f\"\\n   Interpretation:\")\n",
    "    print(f\"      ‚Ä¢ Ultra-rare private mutations\")\n",
    "    print(f\"      ‚Ä¢ Not seen in gnomAD populations\")\n",
    "    print(f\"      ‚Ä¢ Potentially novel disease variants\")\n",
    "\n",
    "# Show gnomAD-only missense\n",
    "print(f\"\\nüìã GNOMAD-ONLY MISSENSE (no clinical annotation):\")\n",
    "print(f\"   Count: {len(gnomad_only)}\")\n",
    "\n",
    "if len(gnomad_only) > 0:\n",
    "    # Check which AF column exists in gnomad_only\n",
    "    af_col_gnomad = None\n",
    "    for col_name in ['AF_gnomad', 'AF']:\n",
    "        if col_name in gnomad_only.columns:\n",
    "            af_col_gnomad = col_name\n",
    "            break\n",
    "    \n",
    "    if af_col_gnomad:\n",
    "        af_series = pd.to_numeric(gnomad_only[af_col_gnomad], errors='coerce').dropna()\n",
    "        if len(af_series) > 0:\n",
    "            print(f\"   Mean AF: {af_series.mean():.6f}\")\n",
    "            print(f\"   Median AF: {af_series.median():.6f}\")\n",
    "            rare_count = (af_series < 0.001).sum()\n",
    "            common_count = (af_series >= 0.01).sum()\n",
    "            print(f\"   Rare variants (AF < 0.1%): {rare_count}\")\n",
    "            print(f\"   Common variants (AF ‚â• 1%): {common_count}\")\n",
    "    \n",
    "    print(f\"\\n   Interpretation:\")\n",
    "    print(f\"      ‚Ä¢ Likely benign population variants\")\n",
    "    print(f\"      ‚Ä¢ VUS candidates needing clinical interpretation\")\n",
    "    print(f\"      ‚Ä¢ Common polymorphisms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSUMMARY:\")\n",
    "print(f\"‚úì ClinVar missense: {len(clinvar_missense)}\")\n",
    "print(f\"‚úì gnomAD missense: {len(gnomad_missense_unique)}\")\n",
    "print(f\"‚úì Overlap: {len(both)} ({len(both)/len(merged_missense)*100:.1f}%)\")\n",
    "print(f\"‚úì ClinVar pathogenic in overlap: {both['is_pathogenic'].sum() if 'is_pathogenic' in both.columns else 'N/A'}\")\n",
    "print(f\"‚úì Gold standard training pairs: {len(both)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROCESSING GNOMAD JSON\n",
      "================================================================================\n",
      "\n",
      "üìÇ Loading: .cache/gnomAD_test_HBB.json\n",
      "   Total rows: 8946\n",
      "\n",
      "üîç Filtering for missense variants...\n",
      "   Missense variants: 1057\n",
      "\n",
      "üß¨ Parsing HGVSp notation...\n",
      "   Successfully parsed: 1057\n",
      "   Failed to parse: 0\n",
      "\n",
      "üìå Filtering for canonical transcripts...\n",
      "   Before: 1057\n",
      "   After: 432\n",
      "\n",
      "üî¨ Fetching protein sequences from Ensembl...\n",
      "   Unique transcripts: 2\n",
      "   [1/2] Fetching ENST00000335295...\n",
      "   [2/2] Fetching NM_000518.5...\n",
      "  ‚ö†Ô∏è  Error fetching NM_000518.5: 400 Client Error: Bad Request for url: https://rest.ensembl.org/sequence/id/NM_000518.5?type=protein\n",
      "\n",
      "‚úÖ Verifying protein sequences...\n",
      "   Verified matches: 216\n",
      "   Mismatches: 0\n",
      "   Unable to verify: 216\n",
      "\n",
      "üîÑ Removing duplicate genomic positions...\n",
      "   Before: 432\n",
      "   After: 170\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "‚úì Total missense variants: 170\n",
      "‚úì Genes: 1\n",
      "‚úì Transcripts: 1\n",
      "‚úì With protein sequences: 170\n",
      "================================================================================\n",
      "\n",
      "üíæ Saving results...\n",
      "   ‚úì CSV: .cache/HBB_protein_level.csv\n",
      "   ‚úì JSON: .cache/HBB_protein_level.json\n",
      "\n",
      "üìä Sample output:\n",
      "CHROM     POS mutation                         HGVSp           AF         Feature\n",
      "   11 5225611    H144R ENSP00000333994.3:p.His144Arg 6.841000e-07 ENST00000335295\n",
      "   11 5225612    H144Y ENSP00000333994.3:p.His144Tyr 6.572800e-06 ENST00000335295\n",
      "   11 5225615    A143T ENSP00000333994.3:p.Ala143Thr 1.368200e-06 ENST00000335295\n",
      "   11 5225626    A139V ENSP00000333994.3:p.Ala139Val 6.841000e-07 ENST00000335295\n",
      "   11 5225630    V138M ENSP00000333994.3:p.Val138Met 6.841000e-07 ENST00000335295\n",
      "   11 5225632    G137D ENSP00000333994.3:p.Gly137Asp 6.571300e-06 ENST00000335295\n",
      "   11 5225635    A136V ENSP00000333994.3:p.Ala136Val 4.788800e-06 ENST00000335295\n",
      "   11 5225638    V135A ENSP00000333994.3:p.Val135Ala 4.600660e-05 ENST00000335295\n",
      "   11 5225638    V135G ENSP00000333994.3:p.Val135Gly 6.841000e-07 ENST00000335295\n",
      "   11 5225638    V135E ENSP00000333994.3:p.Val135Glu 6.572400e-06 ENST00000335295\n",
      "\n",
      "‚úÖ DONE!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from typing import Dict, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def parse_hgvsp_notation(hgvsp: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Parse HGVSp notation to extract ref_aa, position, alt_aa\n",
    "    \n",
    "    Handles formats:\n",
    "        ENSP00000333994.3:p.Glu6Val ‚Üí {'ref_aa': 'E', 'pos': 6, 'alt_aa': 'V'}\n",
    "        p.Glu6Val ‚Üí {'ref_aa': 'E', 'pos': 6, 'alt_aa': 'V'}\n",
    "        NP_000509.1:p.His144Arg ‚Üí {'ref_aa': 'H', 'pos': 144, 'alt_aa': 'R'}\n",
    "    \"\"\"\n",
    "    if pd.isna(hgvsp) or hgvsp == '':\n",
    "        return None\n",
    "    \n",
    "    # Three-letter to one-letter amino acid code\n",
    "    aa_3to1 = {\n",
    "        'Ala': 'A', 'Arg': 'R', 'Asn': 'N', 'Asp': 'D', 'Cys': 'C',\n",
    "        'Gln': 'Q', 'Glu': 'E', 'Gly': 'G', 'His': 'H', 'Ile': 'I',\n",
    "        'Leu': 'L', 'Lys': 'K', 'Met': 'M', 'Phe': 'F', 'Pro': 'P',\n",
    "        'Ser': 'S', 'Thr': 'T', 'Trp': 'W', 'Tyr': 'Y', 'Val': 'V',\n",
    "        'Ter': '*', 'Stop': '*', 'Sec': 'U', 'Pyl': 'O'\n",
    "    }\n",
    "    \n",
    "    # Strip protein ID prefix if present (ENSP...:p. or NP_...:p.)\n",
    "    hgvsp_str = str(hgvsp)\n",
    "    if ':p.' in hgvsp_str:\n",
    "        hgvsp_str = 'p.' + hgvsp_str.split(':p.')[1]\n",
    "    \n",
    "    # Match: p.Glu6Val or p.E6V or p.Glu6*\n",
    "    pattern = r'p\\.([A-Z][a-z]{2}|[A-Z\\*])(\\d+)([A-Z][a-z]{2}|[A-Z\\*\\=])'\n",
    "    match = re.match(pattern, hgvsp_str)\n",
    "    \n",
    "    if match:\n",
    "        ref_aa = match.group(1)\n",
    "        pos = int(match.group(2))\n",
    "        alt_aa = match.group(3)\n",
    "        \n",
    "        # Convert to single letter\n",
    "        ref_1letter = aa_3to1.get(ref_aa, ref_aa)\n",
    "        alt_1letter = aa_3to1.get(alt_aa, alt_aa) if alt_aa != '=' else ref_aa\n",
    "        \n",
    "        return {\n",
    "            'ref_aa': ref_1letter,\n",
    "            'pos': pos,\n",
    "            'alt_aa': alt_1letter\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_protein_sequence_from_ensembl(transcript_id: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Fetch protein sequence from Ensembl REST API\n",
    "    \"\"\"\n",
    "    server = \"https://rest.ensembl.org\"\n",
    "    ext = f\"/sequence/id/{transcript_id}?type=protein\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            server + ext, \n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=10\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data['seq']\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  Error fetching {transcript_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_gnomad_json(json_path: str, \n",
    "                        fetch_sequences: bool = True,\n",
    "                        canonical_only: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process gnomAD JSON file to extract protein-level information\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to gnomAD JSON file\n",
    "        fetch_sequences: Whether to fetch protein sequences from Ensembl\n",
    "        canonical_only: Keep only canonical transcripts\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with protein-level variant information\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PROCESSING GNOMAD JSON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load JSON\n",
    "    print(f\"\\nüìÇ Loading: {json_path}\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"   Total rows: {len(df)}\")\n",
    "    \n",
    "    # Filter for missense variants only\n",
    "    print(f\"\\nüîç Filtering for missense variants...\")\n",
    "    missense = df[\n",
    "        df['Consequence'].str.contains('missense', case=False, na=False)\n",
    "    ].copy()\n",
    "    print(f\"   Missense variants: {len(missense)}\")\n",
    "    \n",
    "    if len(missense) == 0:\n",
    "        print(\"‚ùå No missense variants found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Parse HGVSp notation\n",
    "    print(f\"\\nüß¨ Parsing HGVSp notation...\")\n",
    "    missense['protein_change'] = missense['HGVSp'].apply(parse_hgvsp_notation)\n",
    "    \n",
    "    # Extract individual fields\n",
    "    missense['ref_aa'] = missense['protein_change'].apply(\n",
    "        lambda x: x['ref_aa'] if x else None\n",
    "    )\n",
    "    missense['protein_pos'] = missense['protein_change'].apply(\n",
    "        lambda x: x['pos'] if x else None\n",
    "    )\n",
    "    missense['alt_aa'] = missense['protein_change'].apply(\n",
    "        lambda x: x['alt_aa'] if x else None\n",
    "    )\n",
    "    \n",
    "    # Create mutation string\n",
    "    missense['mutation'] = missense.apply(\n",
    "        lambda row: f\"{row['ref_aa']}{row['protein_pos']}{row['alt_aa']}\" \n",
    "        if row['ref_aa'] else None, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove failed parses\n",
    "    parsed = missense[missense['ref_aa'].notna()].copy()\n",
    "    print(f\"   Successfully parsed: {len(parsed)}\")\n",
    "    print(f\"   Failed to parse: {len(missense) - len(parsed)}\")\n",
    "    \n",
    "    # Filter for canonical transcripts if requested\n",
    "    if canonical_only:\n",
    "        print(f\"\\nüìå Filtering for canonical transcripts...\")\n",
    "        before = len(parsed)\n",
    "        parsed = parsed[parsed['CANONICAL'] == 'YES'].copy()\n",
    "        print(f\"   Before: {before}\")\n",
    "        print(f\"   After: {len(parsed)}\")\n",
    "    \n",
    "    # Select key columns\n",
    "    key_columns = [\n",
    "        'CHROM', 'POS', 'REF', 'ALT',\n",
    "        'SYMBOL', 'Gene', 'Feature', 'Feature_type',\n",
    "        'CANONICAL', 'BIOTYPE',\n",
    "        'ref_aa', 'protein_pos', 'alt_aa', 'mutation',\n",
    "        'HGVSp', 'HGVSc', 'Amino_acids', 'Codons',\n",
    "        'AF', 'AC', 'AN', 'IMPACT', 'Consequence'\n",
    "    ]\n",
    "    \n",
    "    # Add optional columns if they exist\n",
    "    optional_columns = ['MANE_SELECT', 'SIFT_pred', 'PolyPhen_pred', 'EXON', 'INTRON']\n",
    "    for col in optional_columns:\n",
    "        if col in parsed.columns:\n",
    "            key_columns.append(col)\n",
    "    \n",
    "    # Select available columns\n",
    "    available_columns = [col for col in key_columns if col in parsed.columns]\n",
    "    result = parsed[available_columns].copy()\n",
    "    \n",
    "    # Fetch protein sequences if requested\n",
    "    if fetch_sequences:\n",
    "        print(f\"\\nüî¨ Fetching protein sequences from Ensembl...\")\n",
    "        \n",
    "        # Get unique transcripts\n",
    "        unique_transcripts = result['Feature'].dropna().unique()\n",
    "        print(f\"   Unique transcripts: {len(unique_transcripts)}\")\n",
    "        \n",
    "        # Fetch sequences\n",
    "        transcript_to_seq = {}\n",
    "        for i, transcript_id in enumerate(unique_transcripts, 1):\n",
    "            print(f\"   [{i}/{len(unique_transcripts)}] Fetching {transcript_id}...\")\n",
    "            seq = get_protein_sequence_from_ensembl(transcript_id)\n",
    "            transcript_to_seq[transcript_id] = seq\n",
    "            time.sleep(0.1)  # Rate limiting\n",
    "        \n",
    "        # Map to dataframe\n",
    "        result['protein_seq'] = result['Feature'].map(transcript_to_seq)\n",
    "        \n",
    "        # Verify sequences\n",
    "        print(f\"\\n‚úÖ Verifying protein sequences...\")\n",
    "        def verify_match(row):\n",
    "            if pd.isna(row['protein_seq']) or pd.isna(row['protein_pos']):\n",
    "                return None\n",
    "            try:\n",
    "                actual_aa = row['protein_seq'][row['protein_pos'] - 1]\n",
    "                expected_aa = row['ref_aa']\n",
    "                return actual_aa == expected_aa\n",
    "            except:\n",
    "                return None\n",
    "        \n",
    "        result['seq_verified'] = result.apply(verify_match, axis=1)\n",
    "        \n",
    "        verified = result['seq_verified'].sum()\n",
    "        failed = (result['seq_verified'] == False).sum()\n",
    "        missing = result['seq_verified'].isna().sum()\n",
    "        \n",
    "        print(f\"   Verified matches: {verified}\")\n",
    "        print(f\"   Mismatches: {failed}\")\n",
    "        print(f\"   Unable to verify: {missing}\")\n",
    "        \n",
    "        if failed > 0:\n",
    "            print(f\"\\n   ‚ö†Ô∏è  {failed} mismatches found (likely due to isoforms/versions)\")\n",
    "    \n",
    "    # Remove duplicates at genomic level\n",
    "    print(f\"\\nüîÑ Removing duplicate genomic positions...\")\n",
    "    before_dedup = len(result)\n",
    "    result_unique = result.drop_duplicates(subset=['CHROM', 'POS', 'REF', 'ALT']).copy()\n",
    "    print(f\"   Before: {before_dedup}\")\n",
    "    print(f\"   After: {len(result_unique)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úì Total missense variants: {len(result_unique)}\")\n",
    "    print(f\"‚úì Genes: {result_unique['SYMBOL'].nunique()}\")\n",
    "    print(f\"‚úì Transcripts: {result_unique['Feature'].nunique()}\")\n",
    "    if 'protein_seq' in result_unique.columns:\n",
    "        print(f\"‚úì With protein sequences: {result_unique['protein_seq'].notna().sum()}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return result_unique\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function - Process gnomAD JSON file\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== CONFIGURATION =====\n",
    "    INPUT_JSON = \".cache/gnomAD_test_HBB.json\"  # Change this to your file\n",
    "    OUTPUT_CSV = \".cache/HBB_protein_level.csv\"\n",
    "    OUTPUT_JSON = \".cache/HBB_protein_level.json\"\n",
    "    \n",
    "    FETCH_SEQUENCES = True      # Set to False to skip Ensembl API calls\n",
    "    CANONICAL_ONLY = True       # Set to False to keep all transcripts\n",
    "    \n",
    "    # ===== PROCESS =====\n",
    "    result = process_gnomad_json(\n",
    "        INPUT_JSON,\n",
    "        fetch_sequences=FETCH_SEQUENCES,\n",
    "        canonical_only=CANONICAL_ONLY\n",
    "    )\n",
    "    \n",
    "    if len(result) == 0:\n",
    "        print(\"‚ùå No data to save!\")\n",
    "        return\n",
    "    \n",
    "    # ===== SAVE =====\n",
    "    print(f\"üíæ Saving results...\")\n",
    "    result.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"   ‚úì CSV: {OUTPUT_CSV}\")\n",
    "    \n",
    "    result.to_json(OUTPUT_JSON, orient='records', indent=2)\n",
    "    print(f\"   ‚úì JSON: {OUTPUT_JSON}\")\n",
    "    \n",
    "    # ===== DISPLAY SAMPLE =====\n",
    "    print(f\"\\nüìä Sample output:\")\n",
    "    display_cols = ['CHROM', 'POS', 'mutation', 'HGVSp', 'AF', 'Feature']\n",
    "    available_display = [col for col in display_cols if col in result.columns]\n",
    "    print(result[available_display].head(10).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n‚úÖ DONE!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missense: 1057\n",
      "\n",
      "Columns available:\n",
      "['CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'AC', 'AF', 'vep', 'index', 'Allele', 'Consequence', 'IMPACT', 'SYMBOL', 'Gene', 'Feature_type', 'Feature', 'BIOTYPE', 'EXON', 'INTRON', 'HGVSc', 'HGVSp', 'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', 'ALLELE_NUM', 'DISTANCE', 'STRAND', 'FLAGS', 'VARIANT_CLASS', 'SYMBOL_SOURCE', 'HGNC_ID', 'CANONICAL', 'MANE_SELECT', 'MANE_PLUS_CLINICAL', 'TSL', 'APPRIS', 'CCDS', 'ENSP', 'UNIPROT_ISOFORM', 'SOURCE', 'DOMAINS', 'miRNA', 'HGVS_OFFSET', 'PUBMED', 'MOTIF_NAME', 'MOTIF_POS', 'HIGH_INF_POS', 'MOTIF_SCORE_CHANGE', 'TRANSCRIPTION_FACTORS', 'LoF', 'LoF_filter', 'LoF_flags', 'LoF_info']\n",
      "\n",
      "Sample HGVSp values:\n",
      "['ENSP00000333994.3:p.His144Arg', 'ENSP00000494175.1:p.His144Arg', 'NP_000509.1:p.His144Arg', 'ENSP00000333994.3:p.His144Tyr', 'ENSP00000494175.1:p.His144Tyr', 'NP_000509.1:p.His144Tyr', 'ENSP00000333994.3:p.His144Tyr', 'ENSP00000494175.1:p.His144Tyr', 'NP_000509.1:p.His144Tyr', 'ENSP00000333994.3:p.Ala143Thr', 'ENSP00000494175.1:p.Ala143Thr', 'NP_000509.1:p.Ala143Thr', 'ENSP00000333994.3:p.Ala139Val', 'ENSP00000494175.1:p.Ala139Val', 'NP_000509.1:p.Ala139Val', 'ENSP00000333994.3:p.Val138Met', 'ENSP00000494175.1:p.Val138Met', 'NP_000509.1:p.Val138Met', 'ENSP00000333994.3:p.Gly137Asp', 'ENSP00000494175.1:p.Gly137Asp']\n",
      "\n",
      "HGVSp data types:\n",
      "HGVSp\n",
      "<class 'str'>    1057\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows:\n",
      "           Consequence                          HGVSp Amino_acids  \\\n",
      "1380  missense_variant  ENSP00000333994.3:p.His144Arg         H/R   \n",
      "1387  missense_variant  ENSP00000494175.1:p.His144Arg         H/R   \n",
      "1388  missense_variant        NP_000509.1:p.His144Arg         H/R   \n",
      "1390  missense_variant  ENSP00000333994.3:p.His144Tyr         H/Y   \n",
      "1397  missense_variant  ENSP00000494175.1:p.His144Tyr         H/Y   \n",
      "1398  missense_variant        NP_000509.1:p.His144Tyr         H/Y   \n",
      "1400  missense_variant  ENSP00000333994.3:p.His144Tyr         H/Y   \n",
      "1407  missense_variant  ENSP00000494175.1:p.His144Tyr         H/Y   \n",
      "1408  missense_variant        NP_000509.1:p.His144Tyr         H/Y   \n",
      "1410  missense_variant  ENSP00000333994.3:p.Ala143Thr         A/T   \n",
      "\n",
      "              Feature  \n",
      "1380  ENST00000335295  \n",
      "1387  ENST00000647020  \n",
      "1388      NM_000518.5  \n",
      "1390  ENST00000335295  \n",
      "1397  ENST00000647020  \n",
      "1398      NM_000518.5  \n",
      "1400  ENST00000335295  \n",
      "1407  ENST00000647020  \n",
      "1408      NM_000518.5  \n",
      "1410  ENST00000335295  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Quick diagnostic\n",
    "with open('.cache/gnomAD_test_HBB.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check missense variants\n",
    "missense = df[df['Consequence'].str.contains('missense', case=False, na=False)]\n",
    "\n",
    "print(f\"Total missense: {len(missense)}\")\n",
    "print(f\"\\nColumns available:\")\n",
    "print(missense.columns.tolist())\n",
    "\n",
    "print(f\"\\nSample HGVSp values:\")\n",
    "print(missense['HGVSp'].head(20).tolist())\n",
    "\n",
    "print(f\"\\nHGVSp data types:\")\n",
    "print(missense['HGVSp'].apply(type).value_counts())\n",
    "\n",
    "print(f\"\\nSample rows:\")\n",
    "print(missense[['Consequence', 'HGVSp', 'Amino_acids', 'Feature']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE DATA VALIDATION\n",
      "================================================================================\n",
      "\n",
      "üìÇ Loading: .cache/HBB_protein_level.csv\n",
      "   Total rows: 170\n",
      "\n",
      "1Ô∏è‚É£  CHECKING REQUIRED COLUMNS...\n",
      "   ‚úÖ All required columns present\n",
      "\n",
      "2Ô∏è‚É£  CHECKING FOR NULL VALUES...\n",
      "   ‚úÖ ref_aa: no nulls\n",
      "   ‚úÖ protein_pos: no nulls\n",
      "   ‚úÖ alt_aa: no nulls\n",
      "   ‚úÖ protein_seq: no nulls\n",
      "\n",
      "3Ô∏è‚É£  VALIDATING AMINO ACIDS...\n",
      "   ‚úÖ All ref_aa valid\n",
      "   ‚úÖ All alt_aa valid\n",
      "\n",
      "4Ô∏è‚É£  VERIFYING PROTEIN SEQUENCES MATCH...\n",
      "   ‚úÖ Verified matches: 170/170\n",
      "\n",
      "5Ô∏è‚É£  VALIDATING MUTATION STRINGS...\n",
      "   ‚úÖ All mutation strings valid\n",
      "   ‚úÖ All mutation strings match components\n",
      "\n",
      "6Ô∏è‚É£  VALIDATING GENOMIC COORDINATES...\n",
      "   ‚úÖ All chromosomes valid\n",
      "   ‚úÖ All positions > 0\n",
      "\n",
      "7Ô∏è‚É£  CHECKING FOR DUPLICATES...\n",
      "   ‚úÖ No genomic duplicates\n",
      "   ‚ö†Ô∏è  Protein duplicates: 8\n",
      "\n",
      "8Ô∏è‚É£  VALIDATING ALLELE FREQUENCIES...\n",
      "   ‚úÖ All AF in range [0, 1]\n",
      "\n",
      "   AF Distribution:\n",
      "      Ultra-rare (< 0.00001): 139\n",
      "      Rare (0.00001-0.001): 29\n",
      "      Low freq (0.001-0.01): 1\n",
      "      Common (> 0.01): 1\n",
      "\n",
      "9Ô∏è‚É£  CHECKING TRANSCRIPT CONSISTENCY...\n",
      "   Unique transcripts: 1\n",
      "   ‚úÖ Single transcript (canonical)\n",
      "\n",
      "üîü STATISTICAL SUMMARY...\n",
      "   Protein position range: 2 - 144\n",
      "   Protein length: 147\n",
      "   Unique positions affected: 98\n",
      "   Unique mutations: 166\n",
      "\n",
      "   Most common substitutions:\n",
      "      A‚ÜíV: 9\n",
      "      A‚ÜíT: 8\n",
      "      G‚ÜíD: 8\n",
      "      G‚ÜíS: 7\n",
      "      V‚ÜíM: 7\n",
      "\n",
      "================================================================================\n",
      "VALIDATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  WARNINGS (1):\n",
      "   1. 8 duplicate protein mutations\n",
      "\n",
      "   Action required: Review and fix errors before proceeding\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def validate_protein_level_data(csv_path: str):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of protein-level missense data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE DATA VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"\\nüìÇ Loading: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"   Total rows: {len(df)}\")\n",
    "    \n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # ============================================\n",
    "    # 1. CHECK REQUIRED COLUMNS\n",
    "    # ============================================\n",
    "    print(\"\\n1Ô∏è‚É£  CHECKING REQUIRED COLUMNS...\")\n",
    "    required_cols = [\n",
    "        'CHROM', 'POS', 'REF', 'ALT',\n",
    "        'ref_aa', 'protein_pos', 'alt_aa', 'mutation',\n",
    "        'Feature', 'protein_seq'\n",
    "    ]\n",
    "    \n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        errors.append(f\"Missing columns: {missing_cols}\")\n",
    "        print(f\"   ‚ùå Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All required columns present\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 2. CHECK FOR NULL VALUES\n",
    "    # ============================================\n",
    "    print(\"\\n2Ô∏è‚É£  CHECKING FOR NULL VALUES...\")\n",
    "    critical_cols = ['ref_aa', 'protein_pos', 'alt_aa', 'protein_seq']\n",
    "    \n",
    "    for col in critical_cols:\n",
    "        if col in df.columns:\n",
    "            null_count = df[col].isna().sum()\n",
    "            if null_count > 0:\n",
    "                errors.append(f\"{col} has {null_count} null values\")\n",
    "                print(f\"   ‚ùå {col}: {null_count} nulls\")\n",
    "            else:\n",
    "                print(f\"   ‚úÖ {col}: no nulls\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 3. VALIDATE AMINO ACIDS\n",
    "    # ============================================\n",
    "    print(\"\\n3Ô∏è‚É£  VALIDATING AMINO ACIDS...\")\n",
    "    valid_aas = set('ACDEFGHIKLMNPQRSTVWY*')\n",
    "    \n",
    "    invalid_ref = df[~df['ref_aa'].isin(valid_aas)]\n",
    "    invalid_alt = df[~df['alt_aa'].isin(valid_aas)]\n",
    "    \n",
    "    if len(invalid_ref) > 0:\n",
    "        errors.append(f\"{len(invalid_ref)} invalid ref_aa\")\n",
    "        print(f\"   ‚ùå Invalid ref_aa: {invalid_ref['ref_aa'].unique()}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All ref_aa valid\")\n",
    "    \n",
    "    if len(invalid_alt) > 0:\n",
    "        errors.append(f\"{len(invalid_alt)} invalid alt_aa\")\n",
    "        print(f\"   ‚ùå Invalid alt_aa: {invalid_alt['alt_aa'].unique()}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All alt_aa valid\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 4. VERIFY PROTEIN SEQUENCES\n",
    "    # ============================================\n",
    "    print(\"\\n4Ô∏è‚É£  VERIFYING PROTEIN SEQUENCES MATCH...\")\n",
    "    \n",
    "    mismatches = []\n",
    "    seq_errors = []\n",
    "    verified = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row['protein_seq']) and pd.notna(row['protein_pos']):\n",
    "            try:\n",
    "                # Check position is within bounds\n",
    "                if row['protein_pos'] < 1 or row['protein_pos'] > len(row['protein_seq']):\n",
    "                    seq_errors.append(f\"Row {idx}: position {row['protein_pos']} out of bounds (seq length: {len(row['protein_seq'])})\")\n",
    "                    continue\n",
    "                \n",
    "                # Check ref_aa matches sequence\n",
    "                actual_aa = row['protein_seq'][row['protein_pos'] - 1]\n",
    "                expected_aa = row['ref_aa']\n",
    "                \n",
    "                if actual_aa == expected_aa:\n",
    "                    verified += 1\n",
    "                else:\n",
    "                    mismatches.append({\n",
    "                        'row': idx,\n",
    "                        'position': row['protein_pos'],\n",
    "                        'expected': expected_aa,\n",
    "                        'actual': actual_aa,\n",
    "                        'mutation': row['mutation']\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                seq_errors.append(f\"Row {idx}: {e}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Verified matches: {verified}/{len(df)}\")\n",
    "    \n",
    "    if mismatches:\n",
    "        errors.append(f\"{len(mismatches)} sequence mismatches\")\n",
    "        print(f\"   ‚ùå Sequence mismatches: {len(mismatches)}\")\n",
    "        print(f\"\\n   Sample mismatches:\")\n",
    "        for mm in mismatches[:5]:\n",
    "            print(f\"      Row {mm['row']}: {mm['mutation']} - Expected {mm['expected']} but got {mm['actual']} at pos {mm['position']}\")\n",
    "    \n",
    "    if seq_errors:\n",
    "        errors.append(f\"{len(seq_errors)} sequence validation errors\")\n",
    "        print(f\"   ‚ùå Validation errors: {len(seq_errors)}\")\n",
    "        for err in seq_errors[:5]:\n",
    "            print(f\"      {err}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 5. CHECK MUTATION STRING FORMAT\n",
    "    # ============================================\n",
    "    print(\"\\n5Ô∏è‚É£  VALIDATING MUTATION STRINGS...\")\n",
    "    \n",
    "    mutation_pattern = r'^[ACDEFGHIKLMNPQRSTVWY\\*]\\d+[ACDEFGHIKLMNPQRSTVWY\\*]$'\n",
    "    invalid_mutations = df[~df['mutation'].str.match(mutation_pattern, na=False)]\n",
    "    \n",
    "    if len(invalid_mutations) > 0:\n",
    "        errors.append(f\"{len(invalid_mutations)} invalid mutation formats\")\n",
    "        print(f\"   ‚ùå Invalid formats: {len(invalid_mutations)}\")\n",
    "        print(f\"      Examples: {invalid_mutations['mutation'].head().tolist()}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All mutation strings valid\")\n",
    "    \n",
    "    # Verify mutation string matches components\n",
    "    mutation_mismatch = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        expected = f\"{row['ref_aa']}{row['protein_pos']}{row['alt_aa']}\"\n",
    "        if row['mutation'] != expected:\n",
    "            mutation_mismatch += 1\n",
    "    \n",
    "    if mutation_mismatch > 0:\n",
    "        errors.append(f\"{mutation_mismatch} mutation strings don't match components\")\n",
    "        print(f\"   ‚ùå Mutation string mismatches: {mutation_mismatch}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All mutation strings match components\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 6. CHECK GENOMIC COORDINATES\n",
    "    # ============================================\n",
    "    print(\"\\n6Ô∏è‚É£  VALIDATING GENOMIC COORDINATES...\")\n",
    "    \n",
    "    # Check chromosomes\n",
    "    valid_chroms = set([str(i) for i in range(1, 23)] + ['X', 'Y', 'M', 'MT'])\n",
    "    invalid_chroms = df[~df['CHROM'].astype(str).isin(valid_chroms)]\n",
    "    \n",
    "    if len(invalid_chroms) > 0:\n",
    "        warnings.append(f\"{len(invalid_chroms)} non-standard chromosomes\")\n",
    "        print(f\"   ‚ö†Ô∏è  Non-standard chroms: {invalid_chroms['CHROM'].unique()}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All chromosomes valid\")\n",
    "    \n",
    "    # Check positions are positive\n",
    "    invalid_pos = df[df['POS'] <= 0]\n",
    "    if len(invalid_pos) > 0:\n",
    "        errors.append(f\"{len(invalid_pos)} invalid positions\")\n",
    "        print(f\"   ‚ùå Invalid positions: {len(invalid_pos)}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All positions > 0\")\n",
    "    \n",
    "    # Check REF/ALT are valid nucleotides\n",
    "    valid_nucs = set('ACGT')\n",
    "    invalid_ref = df[~df['REF'].str.match(r'^[ACGT]+$', na=False)]\n",
    "    invalid_alt = df[~df['ALT'].str.match(r'^[ACGT]+$', na=False)]\n",
    "    \n",
    "    if len(invalid_ref) > 0:\n",
    "        errors.append(f\"{len(invalid_ref)} invalid REF alleles\")\n",
    "        print(f\"   ‚ùå Invalid REF: {len(invalid_ref)}\")\n",
    "    \n",
    "    if len(invalid_alt) > 0:\n",
    "        errors.append(f\"{len(invalid_alt)} invalid ALT alleles\")\n",
    "        print(f\"   ‚ùå Invalid ALT: {len(invalid_alt)}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 7. CHECK FOR DUPLICATES\n",
    "    # ============================================\n",
    "    print(\"\\n7Ô∏è‚É£  CHECKING FOR DUPLICATES...\")\n",
    "    \n",
    "    # Genomic duplicates\n",
    "    genomic_dups = df[df.duplicated(subset=['CHROM', 'POS', 'REF', 'ALT'], keep=False)]\n",
    "    if len(genomic_dups) > 0:\n",
    "        errors.append(f\"{len(genomic_dups)} duplicate genomic positions\")\n",
    "        print(f\"   ‚ùå Genomic duplicates: {len(genomic_dups)}\")\n",
    "        print(f\"      Example: {genomic_dups[['CHROM', 'POS', 'REF', 'ALT']].head()}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No genomic duplicates\")\n",
    "    \n",
    "    # Protein duplicates (same mutation)\n",
    "    protein_dups = df[df.duplicated(subset=['Feature', 'mutation'], keep=False)]\n",
    "    if len(protein_dups) > 0:\n",
    "        warnings.append(f\"{len(protein_dups)} duplicate protein mutations\")\n",
    "        print(f\"   ‚ö†Ô∏è  Protein duplicates: {len(protein_dups)}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No protein duplicates\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 8. VALIDATE ALLELE FREQUENCIES\n",
    "    # ============================================\n",
    "    print(\"\\n8Ô∏è‚É£  VALIDATING ALLELE FREQUENCIES...\")\n",
    "    \n",
    "    if 'AF' in df.columns:\n",
    "        af_series = pd.to_numeric(df['AF'], errors='coerce')\n",
    "        \n",
    "        # Check range [0, 1]\n",
    "        invalid_af = df[(af_series < 0) | (af_series > 1)]\n",
    "        if len(invalid_af) > 0:\n",
    "            errors.append(f\"{len(invalid_af)} AF values out of range [0,1]\")\n",
    "            print(f\"   ‚ùå Invalid AF range: {len(invalid_af)}\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ All AF in range [0, 1]\")\n",
    "        \n",
    "        # Show distribution\n",
    "        print(f\"\\n   AF Distribution:\")\n",
    "        print(f\"      Ultra-rare (< 0.00001): {(af_series < 0.00001).sum()}\")\n",
    "        print(f\"      Rare (0.00001-0.001): {((af_series >= 0.00001) & (af_series < 0.001)).sum()}\")\n",
    "        print(f\"      Low freq (0.001-0.01): {((af_series >= 0.001) & (af_series < 0.01)).sum()}\")\n",
    "        print(f\"      Common (> 0.01): {(af_series >= 0.01).sum()}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 9. CHECK TRANSCRIPT CONSISTENCY\n",
    "    # ============================================\n",
    "    print(\"\\n9Ô∏è‚É£  CHECKING TRANSCRIPT CONSISTENCY...\")\n",
    "    \n",
    "    unique_transcripts = df['Feature'].nunique()\n",
    "    print(f\"   Unique transcripts: {unique_transcripts}\")\n",
    "    \n",
    "    if unique_transcripts > 1:\n",
    "        warnings.append(f\"Multiple transcripts: {df['Feature'].unique().tolist()}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Multiple transcripts (expected if not canonical_only)\")\n",
    "        for transcript in df['Feature'].unique():\n",
    "            count = (df['Feature'] == transcript).sum()\n",
    "            print(f\"      {transcript}: {count} variants\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Single transcript (canonical)\")\n",
    "    \n",
    "    # Check all variants from same transcript have same sequence\n",
    "    for transcript in df['Feature'].unique():\n",
    "        transcript_df = df[df['Feature'] == transcript]\n",
    "        unique_seqs = transcript_df['protein_seq'].nunique()\n",
    "        if unique_seqs > 1:\n",
    "            errors.append(f\"Transcript {transcript} has {unique_seqs} different sequences\")\n",
    "            print(f\"   ‚ùå {transcript}: {unique_seqs} different sequences!\")\n",
    "    \n",
    "    # ============================================\n",
    "    # 10. STATISTICAL SUMMARY\n",
    "    # ============================================\n",
    "    print(\"\\nüîü STATISTICAL SUMMARY...\")\n",
    "    \n",
    "    print(f\"   Protein position range: {df['protein_pos'].min()} - {df['protein_pos'].max()}\")\n",
    "    print(f\"   Protein length: {len(df['protein_seq'].iloc[0]) if len(df) > 0 else 'N/A'}\")\n",
    "    print(f\"   Unique positions affected: {df['protein_pos'].nunique()}\")\n",
    "    print(f\"   Unique mutations: {df['mutation'].nunique()}\")\n",
    "    \n",
    "    # Most common amino acid changes\n",
    "    print(f\"\\n   Most common substitutions:\")\n",
    "    ref_alt = df['ref_aa'] + '‚Üí' + df['alt_aa']\n",
    "    for sub, count in ref_alt.value_counts().head(5).items():\n",
    "        print(f\"      {sub}: {count}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # FINAL REPORT\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if len(errors) == 0 and len(warnings) == 0:\n",
    "        print(\"\\nüéâ ‚úÖ DATA IS 100% CORRECT!\")\n",
    "        print(f\"\\n   Total validated variants: {len(df)}\")\n",
    "        print(f\"   All checks passed\")\n",
    "        return True\n",
    "    else:\n",
    "        if errors:\n",
    "            print(f\"\\n‚ùå CRITICAL ERRORS ({len(errors)}):\")\n",
    "            for i, err in enumerate(errors, 1):\n",
    "                print(f\"   {i}. {err}\")\n",
    "        \n",
    "        if warnings:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNINGS ({len(warnings)}):\")\n",
    "            for i, warn in enumerate(warnings, 1):\n",
    "                print(f\"   {i}. {warn}\")\n",
    "        \n",
    "        print(f\"\\n   Action required: Review and fix errors before proceeding\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# RUN VALIDATION\n",
    "if __name__ == \"__main__\":\n",
    "    validate_protein_level_data('.cache/HBB_protein_level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-DATABASE VALIDATION: HBB\n",
      "================================================================================\n",
      "\n",
      "üìÇ Loading your data from .cache/HBB_protein_level.csv...\n",
      "   Variants: 170\n",
      "   Transcript: ENST00000335295\n",
      "   Sequence length: 147 aa\n",
      "\n",
      "üß™ Querying Ensembl for HBB...\n",
      "   Gene ID: ENSG00000244734\n",
      "   Canonical transcript: ENST00000335295.4\n",
      "   ‚ùå Error: HTTPSConnectionPool(host='rest.ensembl.org', port=443): Read timed out. (read timeout=10)\n",
      "\n",
      "üî¨ Querying UniProt for HBB...\n",
      "   ‚úÖ Found: P68871\n",
      "   Length: 147 aa\n",
      "\n",
      "üß¨ Querying NCBI RefSeq for HBB...\n",
      "   Gene ID: 3043\n",
      "   ‚ö†Ô∏è  RefSeq protein lookup requires additional parsing\n",
      "\n",
      "================================================================================\n",
      "SEQUENCE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "üîç Comparing Your_Data vs UniProt...\n",
      "   ‚úÖ IDENTICAL (100% match)\n",
      "\n",
      "================================================================================\n",
      "VARIANT VALIDATION\n",
      "================================================================================\n",
      "\n",
      "üß¨ Checking if ref_aa matches protein sequence...\n",
      "   ‚úÖ Your sequence matches UniProt (gold standard)\n",
      "\n",
      "   Validated: 170/170 variants\n",
      "   ‚úÖ All variants validated against UniProt\n",
      "\n",
      "================================================================================\n",
      "TRANSCRIPT VERSION CHECK\n",
      "================================================================================\n",
      "\n",
      "   Your transcript: ENST00000335295\n",
      "\n",
      "================================================================================\n",
      "VALIDATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ All sequences identical across databases\n",
      "‚úÖ All 170 variants validated\n",
      "\n",
      "================================================================================\n",
      "üéâ ‚úÖ DATA IS CONSISTENT ACROSS ALL DATABASES!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from typing import Dict, Optional, List\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "\n",
    "def fetch_uniprot_sequence(gene_symbol: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch canonical protein sequence from UniProt\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Querying UniProt for {gene_symbol}...\")\n",
    "    \n",
    "    # Search for gene\n",
    "    search_url = f\"https://rest.uniprot.org/uniprotkb/search?query=gene:{gene_symbol}+AND+organism_id:9606+AND+reviewed:true&format=json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(search_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'results' not in data or len(data['results']) == 0:\n",
    "            print(f\"   ‚ö†Ô∏è  No UniProt entry found\")\n",
    "            return None\n",
    "        \n",
    "        # Get first (canonical) entry\n",
    "        entry = data['results'][0]\n",
    "        accession = entry['primaryAccession']\n",
    "        sequence = entry['sequence']['value']\n",
    "        length = entry['sequence']['length']\n",
    "        \n",
    "        print(f\"   ‚úÖ Found: {accession}\")\n",
    "        print(f\"   Length: {length} aa\")\n",
    "        \n",
    "        return {\n",
    "            'accession': accession,\n",
    "            'sequence': sequence,\n",
    "            'length': length,\n",
    "            'gene': entry.get('genes', [{}])[0].get('geneName', {}).get('value', gene_symbol)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_refseq_sequence(gene_symbol: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch protein sequence from NCBI RefSeq\n",
    "    \"\"\"\n",
    "    print(f\"\\nüß¨ Querying NCBI RefSeq for {gene_symbol}...\")\n",
    "    \n",
    "    try:\n",
    "        from Bio import Entrez\n",
    "        Entrez.email = \"your.email@example.com\"  # REQUIRED by NCBI\n",
    "        \n",
    "        # Search for gene\n",
    "        search_handle = Entrez.esearch(\n",
    "            db=\"gene\",\n",
    "            term=f\"{gene_symbol}[Gene] AND human[Organism]\",\n",
    "            retmax=1\n",
    "        )\n",
    "        search_results = Entrez.read(search_handle)\n",
    "        search_handle.close()\n",
    "        \n",
    "        if not search_results['IdList']:\n",
    "            print(f\"   ‚ö†Ô∏è  No gene found\")\n",
    "            return None\n",
    "        \n",
    "        gene_id = search_results['IdList'][0]\n",
    "        \n",
    "        # Get gene details\n",
    "        summary_handle = Entrez.esummary(db=\"gene\", id=gene_id)\n",
    "        summary = Entrez.read(summary_handle)\n",
    "        summary_handle.close()\n",
    "        \n",
    "        # Try to get protein accession from gene\n",
    "        # This is simplified - real implementation would need more parsing\n",
    "        print(f\"   Gene ID: {gene_id}\")\n",
    "        print(f\"   ‚ö†Ô∏è  RefSeq protein lookup requires additional parsing\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except ImportError:\n",
    "        print(f\"   ‚ö†Ô∏è  Biopython not installed (pip install biopython)\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_ensembl_gene_info(gene_symbol: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch gene and transcript info from Ensembl\n",
    "    \"\"\"\n",
    "    print(f\"\\nüß™ Querying Ensembl for {gene_symbol}...\")\n",
    "    \n",
    "    try:\n",
    "        # Look up gene\n",
    "        lookup_url = f\"https://rest.ensembl.org/lookup/symbol/homo_sapiens/{gene_symbol}\"\n",
    "        response = requests.get(\n",
    "            lookup_url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=10\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        gene_data = response.json()\n",
    "        \n",
    "        gene_id = gene_data['id']\n",
    "        print(f\"   Gene ID: {gene_id}\")\n",
    "        \n",
    "        # Get canonical transcript\n",
    "        if 'canonical_transcript' in gene_data:\n",
    "            canonical = gene_data['canonical_transcript']\n",
    "            print(f\"   Canonical transcript: {canonical}\")\n",
    "            \n",
    "            # Fetch sequence\n",
    "            seq_url = f\"https://rest.ensembl.org/sequence/id/{canonical}?type=protein\"\n",
    "            seq_response = requests.get(\n",
    "                seq_url,\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                timeout=10\n",
    "            )\n",
    "            seq_response.raise_for_status()\n",
    "            seq_data = seq_response.json()\n",
    "            \n",
    "            sequence = seq_data['seq']\n",
    "            print(f\"   Length: {len(sequence)} aa\")\n",
    "            \n",
    "            return {\n",
    "                'gene_id': gene_id,\n",
    "                'transcript_id': canonical,\n",
    "                'sequence': sequence,\n",
    "                'length': len(sequence),\n",
    "                'chromosome': gene_data.get('seq_region_name'),\n",
    "                'start': gene_data.get('start'),\n",
    "                'end': gene_data.get('end')\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def compare_sequences(seq1: str, seq2: str, name1: str, name2: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Compare two protein sequences\n",
    "    \"\"\"\n",
    "    if seq1 == seq2:\n",
    "        return {\n",
    "            'identical': True,\n",
    "            'identity': 100.0,\n",
    "            'differences': []\n",
    "        }\n",
    "    \n",
    "    # Find differences\n",
    "    differences = []\n",
    "    min_len = min(len(seq1), len(seq2))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if seq1[i] != seq2[i]:\n",
    "            differences.append({\n",
    "                'position': i + 1,\n",
    "                f'{name1}_aa': seq1[i],\n",
    "                f'{name2}_aa': seq2[i]\n",
    "            })\n",
    "    \n",
    "    # Check length differences\n",
    "    if len(seq1) != len(seq2):\n",
    "        differences.append({\n",
    "            'type': 'length_difference',\n",
    "            f'{name1}_length': len(seq1),\n",
    "            f'{name2}_length': len(seq2)\n",
    "        })\n",
    "    \n",
    "    identity = (min_len - len([d for d in differences if 'position' in d])) / min_len * 100\n",
    "    \n",
    "    return {\n",
    "        'identical': False,\n",
    "        'identity': identity,\n",
    "        'differences': differences\n",
    "    }\n",
    "\n",
    "\n",
    "def cross_validate_gene(csv_path: str, gene_symbol: str):\n",
    "    \"\"\"\n",
    "    Cross-validate protein data against multiple databases\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"CROSS-DATABASE VALIDATION: {gene_symbol}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load your data\n",
    "    print(f\"\\nüìÇ Loading your data from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"   Variants: {len(df)}\")\n",
    "    \n",
    "    your_transcript = df['Feature'].iloc[0]\n",
    "    your_sequence = df['protein_seq'].iloc[0]\n",
    "    your_length = len(your_sequence)\n",
    "    \n",
    "    print(f\"   Transcript: {your_transcript}\")\n",
    "    print(f\"   Sequence length: {your_length} aa\")\n",
    "    \n",
    "    # Fetch from databases\n",
    "    databases = {}\n",
    "    \n",
    "    # 1. Ensembl\n",
    "    ensembl_data = fetch_ensembl_gene_info(gene_symbol)\n",
    "    if ensembl_data:\n",
    "        databases['Ensembl'] = ensembl_data\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # 2. UniProt\n",
    "    uniprot_data = fetch_uniprot_sequence(gene_symbol)\n",
    "    if uniprot_data:\n",
    "        databases['UniProt'] = uniprot_data\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # 3. RefSeq\n",
    "    refseq_data = fetch_refseq_sequence(gene_symbol)\n",
    "    if refseq_data:\n",
    "        databases['RefSeq'] = refseq_data\n",
    "    \n",
    "    # ============================================\n",
    "    # COMPARE SEQUENCES\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SEQUENCE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_sequences = {'Your_Data': your_sequence}\n",
    "    \n",
    "    if 'Ensembl' in databases:\n",
    "        all_sequences['Ensembl'] = databases['Ensembl']['sequence']\n",
    "    \n",
    "    if 'UniProt' in databases:\n",
    "        all_sequences['UniProt'] = databases['UniProt']['sequence']\n",
    "    \n",
    "    # Compare all pairs\n",
    "    comparisons = []\n",
    "    db_names = list(all_sequences.keys())\n",
    "    \n",
    "    for i in range(len(db_names)):\n",
    "        for j in range(i + 1, len(db_names)):\n",
    "            name1, name2 = db_names[i], db_names[j]\n",
    "            seq1, seq2 = all_sequences[name1], all_sequences[name2]\n",
    "            \n",
    "            print(f\"\\nüîç Comparing {name1} vs {name2}...\")\n",
    "            comparison = compare_sequences(seq1, seq2, name1, name2)\n",
    "            \n",
    "            if comparison['identical']:\n",
    "                print(f\"   ‚úÖ IDENTICAL (100% match)\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Identity: {comparison['identity']:.2f}%\")\n",
    "                print(f\"   Differences: {len([d for d in comparison['differences'] if 'position' in d])}\")\n",
    "                \n",
    "                # Show first few differences\n",
    "                pos_diffs = [d for d in comparison['differences'] if 'position' in d]\n",
    "                if pos_diffs:\n",
    "                    print(f\"\\n   First 5 differences:\")\n",
    "                    for diff in pos_diffs[:5]:\n",
    "                        print(f\"      Pos {diff['position']}: {diff.get(f'{name1}_aa')} ‚Üí {diff.get(f'{name2}_aa')}\")\n",
    "            \n",
    "            comparisons.append({\n",
    "                'db1': name1,\n",
    "                'db2': name2,\n",
    "                'identical': comparison['identical'],\n",
    "                'identity': comparison['identity'],\n",
    "                'n_differences': len([d for d in comparison['differences'] if 'position' in d])\n",
    "            })\n",
    "    \n",
    "    # ============================================\n",
    "    # VALIDATE VARIANTS\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VARIANT VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüß¨ Checking if ref_aa matches protein sequence...\")\n",
    "    \n",
    "    # Use the most authoritative sequence for validation\n",
    "    reference_seq = your_sequence\n",
    "    reference_db = \"Your_Data\"\n",
    "    \n",
    "    if 'UniProt' in all_sequences and all_sequences['UniProt'] == your_sequence:\n",
    "        print(f\"   ‚úÖ Your sequence matches UniProt (gold standard)\")\n",
    "        reference_seq = all_sequences['UniProt']\n",
    "        reference_db = \"UniProt\"\n",
    "    elif 'Ensembl' in all_sequences and all_sequences['Ensembl'] == your_sequence:\n",
    "        print(f\"   ‚úÖ Your sequence matches Ensembl canonical\")\n",
    "        reference_seq = all_sequences['Ensembl']\n",
    "        reference_db = \"Ensembl\"\n",
    "    \n",
    "    # Validate each variant\n",
    "    mismatches = []\n",
    "    validated = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        pos = row['protein_pos']\n",
    "        ref_aa = row['ref_aa']\n",
    "        \n",
    "        if pos < 1 or pos > len(reference_seq):\n",
    "            mismatches.append({\n",
    "                'row': idx,\n",
    "                'variant': row['mutation'],\n",
    "                'issue': f'Position {pos} out of range (sequence length: {len(reference_seq)})'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        actual_aa = reference_seq[pos - 1]\n",
    "        \n",
    "        if actual_aa == ref_aa:\n",
    "            validated += 1\n",
    "        else:\n",
    "            mismatches.append({\n",
    "                'row': idx,\n",
    "                'variant': row['mutation'],\n",
    "                'expected': ref_aa,\n",
    "                'actual': actual_aa,\n",
    "                'position': pos\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n   Validated: {validated}/{len(df)} variants\")\n",
    "    \n",
    "    if mismatches:\n",
    "        print(f\"   ‚ùå Mismatches: {len(mismatches)}\")\n",
    "        print(f\"\\n   First 10 mismatches:\")\n",
    "        for mm in mismatches[:10]:\n",
    "            if 'issue' in mm:\n",
    "                print(f\"      {mm['variant']}: {mm['issue']}\")\n",
    "            else:\n",
    "                print(f\"      {mm['variant']}: Expected {mm['expected']} but sequence has {mm['actual']} at pos {mm['position']}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All variants validated against {reference_db}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # CHECK TRANSCRIPT VERSIONS\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRANSCRIPT VERSION CHECK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n   Your transcript: {your_transcript}\")\n",
    "    \n",
    "    if 'Ensembl' in databases:\n",
    "        ensembl_transcript = databases['Ensembl']['transcript_id']\n",
    "        print(f\"   Ensembl canonical: {ensembl_transcript}\")\n",
    "        \n",
    "        if your_transcript == ensembl_transcript:\n",
    "            print(f\"   ‚úÖ Using Ensembl canonical transcript\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Different from Ensembl canonical\")\n",
    "    \n",
    "    # ============================================\n",
    "    # FINAL SUMMARY\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check sequence consistency\n",
    "    if len(set(all_sequences.values())) > 1:\n",
    "        issues.append(\"Sequences differ between databases\")\n",
    "        print(f\"\\n‚ö†Ô∏è  Sequence differences detected:\")\n",
    "        for name, seq in all_sequences.items():\n",
    "            print(f\"   {name}: {len(seq)} aa\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ All sequences identical across databases\")\n",
    "    \n",
    "    # Check variant validation\n",
    "    if mismatches:\n",
    "        issues.append(f\"{len(mismatches)} variant mismatches\")\n",
    "        print(f\"‚ùå {len(mismatches)} variants don't match reference sequence\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All {len(df)} variants validated\")\n",
    "    \n",
    "    # Final verdict\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    if len(issues) == 0:\n",
    "        print(\"üéâ ‚úÖ DATA IS CONSISTENT ACROSS ALL DATABASES!\")\n",
    "        print(\"=\"*80)\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  ISSUES DETECTED:\")\n",
    "        for i, issue in enumerate(issues, 1):\n",
    "            print(f\"   {i}. {issue}\")\n",
    "        print(\"=\"*80)\n",
    "        return False\n",
    "\n",
    "\n",
    "# MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    # UPDATE THESE\n",
    "    CSV_PATH = \".cache/HBB_protein_level.csv\"\n",
    "    GENE_SYMBOL = \"HBB\"\n",
    "    \n",
    "    cross_validate_gene(CSV_PATH, GENE_SYMBOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE VERIFICATION: HBB\n",
      "================================================================================\n",
      "\n",
      "üìÇ Your Data:\n",
      "   Transcript: ENST00000335295\n",
      "   Sequence length: 147 aa\n",
      "   Variants: 170\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n",
      "MANE SELECT VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "üìä MANE Select Status:\n",
      "MANE_SELECT\n",
      "NM_000518.5    170\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîç Your transcript: ENST00000335295\n",
      "   MANE Select: NM_000518.5\n",
      "   ‚úÖ This IS a MANE Select transcript\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "ENSEMBL CANONICAL TRANSCRIPT CHECK\n",
      "================================================================================\n",
      "\n",
      "üß¨ Ensembl canonical transcript: ENST00000335295.4\n",
      "\n",
      "üìã Total transcripts for HBB: 7\n",
      "\n",
      "‚ö†Ô∏è  No MANE Select found (may not be available for this gene)\n",
      "\n",
      "   ‚úÖ Your transcript matches Ensembl canonical\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "UNIPROT CANONICAL ISOFORM CHECK\n",
      "================================================================================\n",
      "\n",
      "‚úÖ UniProt entry: P68871\n",
      "   Protein name: Hemoglobin subunit beta\n",
      "\n",
      "üß¨ Canonical sequence length: 147 aa\n",
      "\n",
      "   ‚úÖ Your sequence matches UniProt canonical\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ADDITIONAL CHECKS:\n",
      "   ‚úÖ Sequence starts with Methionine\n",
      "   ‚úÖ No premature stop codons\n",
      "   ‚úÖ All variants within sequence bounds (144 <= 147)\n",
      "\n",
      "================================================================================\n",
      "FINAL VERIFICATION RESULT\n",
      "================================================================================\n",
      "\n",
      "üéâ ‚úÖ COMPLETELY VERIFIED - NO ISSUES FOUND!\n",
      "\n",
      "   Your data is using the correct:\n",
      "   ‚Ä¢ Clinical-grade transcript (MANE Select or Canonical)\n",
      "   ‚Ä¢ Matches UniProt canonical sequence\n",
      "   ‚Ä¢ All variants validated\n",
      "\n",
      "   ‚úÖ SAFE TO PROCEED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def check_mane_status(csv_path: str):\n",
    "    \"\"\"\n",
    "    Verify we're using MANE Select transcript\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"MANE SELECT VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Check if MANE_SELECT column exists\n",
    "    if 'MANE_SELECT' not in df.columns:\n",
    "        print(\"\\n‚ùå CRITICAL: MANE_SELECT column missing!\")\n",
    "        print(\"   Your JSON may not have MANE annotations\")\n",
    "        return False\n",
    "    \n",
    "    # Check MANE Select status\n",
    "    print(f\"\\nüìä MANE Select Status:\")\n",
    "    mane_values = df['MANE_SELECT'].value_counts(dropna=False)\n",
    "    print(mane_values)\n",
    "    \n",
    "    # Check if we're using MANE Select\n",
    "    transcript = df['Feature'].iloc[0]\n",
    "    mane_select = df['MANE_SELECT'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nüîç Your transcript: {transcript}\")\n",
    "    print(f\"   MANE Select: {mane_select}\")\n",
    "    \n",
    "    if pd.notna(mane_select):\n",
    "        print(f\"   ‚úÖ This IS a MANE Select transcript\")\n",
    "        \n",
    "        # Verify it matches\n",
    "        if mane_select.startswith('ENST'):\n",
    "            mane_transcript = mane_select.split('.')[0]  # Remove version\n",
    "            your_transcript = transcript.split('.')[0]\n",
    "            \n",
    "            if mane_transcript == your_transcript:\n",
    "                print(f\"   ‚úÖ MANE Select matches your transcript\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  WARNING: MANE Select is {mane_transcript}, but you have {your_transcript}\")\n",
    "                return False\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  WARNING: No MANE Select annotation\")\n",
    "        print(f\"   Using CANONICAL instead\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def verify_canonical_vs_mane(gene_symbol: str):\n",
    "    \"\"\"\n",
    "    Query Ensembl to verify canonical transcript\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENSEMBL CANONICAL TRANSCRIPT CHECK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Get gene info\n",
    "        url = f\"https://rest.ensembl.org/lookup/symbol/homo_sapiens/{gene_symbol}\"\n",
    "        response = requests.get(\n",
    "            url,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            params={\"expand\": 1},\n",
    "            timeout=30\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        canonical = data.get('canonical_transcript')\n",
    "        print(f\"\\nüß¨ Ensembl canonical transcript: {canonical}\")\n",
    "        \n",
    "        # Get all transcripts\n",
    "        if 'Transcript' in data:\n",
    "            transcripts = data['Transcript']\n",
    "            print(f\"\\nüìã Total transcripts for {gene_symbol}: {len(transcripts)}\")\n",
    "            \n",
    "            # Look for MANE Select\n",
    "            mane_transcripts = []\n",
    "            for t in transcripts:\n",
    "                if t.get('is_mane_select'):\n",
    "                    mane_transcripts.append({\n",
    "                        'id': t['id'],\n",
    "                        'version': t.get('version'),\n",
    "                        'biotype': t.get('biotype'),\n",
    "                        'is_canonical': t['id'] == canonical\n",
    "                    })\n",
    "            \n",
    "            if mane_transcripts:\n",
    "                print(f\"\\n‚úÖ MANE Select transcript(s):\")\n",
    "                for mt in mane_transcripts:\n",
    "                    print(f\"   {mt['id']}.{mt['version']}\")\n",
    "                    print(f\"      Canonical: {mt['is_canonical']}\")\n",
    "                    print(f\"      Biotype: {mt['biotype']}\")\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  No MANE Select found (may not be available for this gene)\")\n",
    "        \n",
    "        return canonical\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error querying Ensembl: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_uniprot_canonical(gene_symbol: str):\n",
    "    \"\"\"\n",
    "    Verify against UniProt canonical sequence\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"UNIPROT CANONICAL ISOFORM CHECK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Search UniProt\n",
    "        search_url = f\"https://rest.uniprot.org/uniprotkb/search?query=gene:{gene_symbol}+AND+organism_id:9606+AND+reviewed:true&format=json\"\n",
    "        response = requests.get(search_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'results' not in data or len(data['results']) == 0:\n",
    "            print(\"   ‚ùå No UniProt entry found\")\n",
    "            return None\n",
    "        \n",
    "        entry = data['results'][0]\n",
    "        accession = entry['primaryAccession']\n",
    "        \n",
    "        print(f\"\\n‚úÖ UniProt entry: {accession}\")\n",
    "        print(f\"   Protein name: {entry.get('proteinDescription', {}).get('recommendedName', {}).get('fullName', {}).get('value', 'N/A')}\")\n",
    "        \n",
    "        # Check for isoforms\n",
    "        if 'comments' in entry:\n",
    "            for comment in entry['comments']:\n",
    "                if comment.get('commentType') == 'ALTERNATIVE PRODUCTS':\n",
    "                    print(f\"\\nüìã Alternative isoforms detected:\")\n",
    "                    isoforms = comment.get('isoforms', [])\n",
    "                    for iso in isoforms:\n",
    "                        print(f\"   {iso.get('name', {}).get('value', 'N/A')}\")\n",
    "        \n",
    "        # Get canonical sequence\n",
    "        sequence = entry['sequence']['value']\n",
    "        length = entry['sequence']['length']\n",
    "        \n",
    "        print(f\"\\nüß¨ Canonical sequence length: {length} aa\")\n",
    "        \n",
    "        return {\n",
    "            'accession': accession,\n",
    "            'sequence': sequence,\n",
    "            'length': length\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error querying UniProt: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def comprehensive_verification(csv_path: str, gene_symbol: str):\n",
    "    \"\"\"\n",
    "    Complete verification against all standards\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"COMPREHENSIVE VERIFICATION: {gene_symbol}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load your data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    your_transcript = df['Feature'].iloc[0]\n",
    "    your_sequence = df['protein_seq'].iloc[0]\n",
    "    \n",
    "    print(f\"\\nüìÇ Your Data:\")\n",
    "    print(f\"   Transcript: {your_transcript}\")\n",
    "    print(f\"   Sequence length: {len(your_sequence)} aa\")\n",
    "    print(f\"   Variants: {len(df)}\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # 1. Check MANE Select\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    if not check_mane_status(csv_path):\n",
    "        issues.append(\"MANE Select verification failed\")\n",
    "    \n",
    "    # 2. Check Ensembl canonical\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    ensembl_canonical = verify_canonical_vs_mane(gene_symbol)\n",
    "    if ensembl_canonical:\n",
    "        your_base = your_transcript.split('.')[0]\n",
    "        ensembl_base = ensembl_canonical.split('.')[0]\n",
    "        \n",
    "        if your_base != ensembl_base:\n",
    "            issues.append(f\"Your transcript {your_base} != Ensembl canonical {ensembl_base}\")\n",
    "            print(f\"\\n   ‚ö†Ô∏è  WARNING: Transcript mismatch!\")\n",
    "        else:\n",
    "            print(f\"\\n   ‚úÖ Your transcript matches Ensembl canonical\")\n",
    "    \n",
    "    # 3. Check UniProt\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    uniprot_data = check_uniprot_canonical(gene_symbol)\n",
    "    if uniprot_data:\n",
    "        if uniprot_data['sequence'] == your_sequence:\n",
    "            print(f\"\\n   ‚úÖ Your sequence matches UniProt canonical\")\n",
    "        else:\n",
    "            issues.append(\"Sequence mismatch with UniProt\")\n",
    "            print(f\"\\n   ‚ùå CRITICAL: Sequence differs from UniProt!\")\n",
    "            print(f\"      Your length: {len(your_sequence)}\")\n",
    "            print(f\"      UniProt length: {uniprot_data['length']}\")\n",
    "    \n",
    "    # 4. Check for common issues\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"ADDITIONAL CHECKS:\")\n",
    "    \n",
    "    # Methionine at position 1\n",
    "    if your_sequence[0] != 'M':\n",
    "        issues.append(f\"Sequence doesn't start with Methionine (starts with {your_sequence[0]})\")\n",
    "        print(f\"   ‚ö†Ô∏è  Sequence starts with {your_sequence[0]}, not M\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ Sequence starts with Methionine\")\n",
    "    \n",
    "    # Stop codon\n",
    "    if '*' in your_sequence[:-1]:  # Stop codon in middle\n",
    "        issues.append(\"Premature stop codon detected\")\n",
    "        print(f\"   ‚ùå Premature stop codon at position {your_sequence.index('*') + 1}\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No premature stop codons\")\n",
    "    \n",
    "    # Check variant positions are all within bounds\n",
    "    max_pos = df['protein_pos'].max()\n",
    "    seq_len = len(your_sequence)\n",
    "    if max_pos > seq_len:\n",
    "        issues.append(f\"Variant at position {max_pos} exceeds sequence length {seq_len}\")\n",
    "        print(f\"   ‚ùå Variants exceed sequence length!\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ All variants within sequence bounds ({max_pos} <= {seq_len})\")\n",
    "    \n",
    "    # Final verdict\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL VERIFICATION RESULT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if len(issues) == 0:\n",
    "        print(\"\\nüéâ ‚úÖ COMPLETELY VERIFIED - NO ISSUES FOUND!\")\n",
    "        print(\"\\n   Your data is using the correct:\")\n",
    "        print(\"   ‚Ä¢ Clinical-grade transcript (MANE Select or Canonical)\")\n",
    "        print(\"   ‚Ä¢ Matches UniProt canonical sequence\")\n",
    "        print(\"   ‚Ä¢ All variants validated\")\n",
    "        print(\"\\n   ‚úÖ SAFE TO PROCEED\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(issues)} ISSUE(S) DETECTED:\")\n",
    "        for i, issue in enumerate(issues, 1):\n",
    "            print(f\"   {i}. {issue}\")\n",
    "        print(\"\\n   ‚ö†Ô∏è  REVIEW REQUIRED BEFORE PROCEEDING\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# RUN COMPREHENSIVE CHECK\n",
    "if __name__ == \"__main__\":\n",
    "    result = comprehensive_verification(\n",
    "        csv_path=\".cache/HBB_protein_level.csv\",\n",
    "        gene_symbol=\"HBB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATION USING GNOMAD'S VEP ANNOTATIONS\n",
      "================================================================================\n",
      "\n",
      "üìä Checking consistency of gnomAD annotations...\n",
      "\n",
      "1Ô∏è‚É£  HGVSc (DNA-level) annotations:\n",
      "   Present: 170/170\n",
      "   Sample: ['ENST00000335295.4:c.431A>G', 'ENST00000335295.4:c.430C>T', 'ENST00000335295.4:c.427G>A']\n",
      "\n",
      "2Ô∏è‚É£  Codon change information:\n",
      "   Present: 170/170\n",
      "   Sample: ['cAc/cGc', 'Cac/Tac', 'Gcc/Acc', 'gCt/gTt', 'Gtg/Atg']\n",
      "\n",
      "3Ô∏è‚É£  Amino acid annotation consistency:\n",
      "   ‚úÖ All amino acid annotations consistent (170 variants)\n",
      "\n",
      "4Ô∏è‚É£  MANE Select annotation:\n",
      "   Variants with MANE: 170/170\n",
      "   MANE transcript: NM_000518.5\n",
      "   ‚úÖ All variants from MANE Select transcript\n",
      "\n",
      "5Ô∏è‚É£  Variant consequence consistency:\n",
      "   ‚úÖ All 170 variants are missense (as expected)\n",
      "   MODERATE impact: 170/170\n",
      "   ‚úÖ Expected for missense variants\n",
      "\n",
      "================================================================================\n",
      "VALIDATION RESULT\n",
      "================================================================================\n",
      "\n",
      "üéâ ‚úÖ ALL GNOMAD ANNOTATIONS ARE INTERNALLY CONSISTENT!\n",
      "\n",
      "   This confirms:\n",
      "   ‚Ä¢ Genomic coordinates are correct (gnomAD QC)\n",
      "   ‚Ä¢ VEP annotations are correct (ran by gnomAD)\n",
      "   ‚Ä¢ Protein changes are correct (validated against UniProt)\n",
      "   ‚Ä¢ Your processing preserved all accuracy\n",
      "\n",
      "   ‚úÖ‚úÖ‚úÖ DATA IS 100% VALIDATED ‚úÖ‚úÖ‚úÖ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_using_existing_annotations(csv_path: str):\n",
    "    \"\"\"\n",
    "    Validate genomic correctness using gnomAD's own annotations\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"VALIDATION USING GNOMAD'S VEP ANNOTATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"\\nüìä Checking consistency of gnomAD annotations...\")\n",
    "    \n",
    "    # Check 1: HGVSc and HGVSp consistency\n",
    "    print(f\"\\n1Ô∏è‚É£  HGVSc (DNA-level) annotations:\")\n",
    "    print(f\"   Present: {df['HGVSc'].notna().sum()}/{len(df)}\")\n",
    "    print(f\"   Sample: {df['HGVSc'].dropna().head(3).tolist()}\")\n",
    "    \n",
    "    # Check 2: Codon information\n",
    "    print(f\"\\n2Ô∏è‚É£  Codon change information:\")\n",
    "    print(f\"   Present: {df['Codons'].notna().sum()}/{len(df)}\")\n",
    "    print(f\"   Sample: {df['Codons'].dropna().head(5).tolist()}\")\n",
    "    \n",
    "    # Check 3: Amino acid consistency\n",
    "    print(f\"\\n3Ô∏è‚É£  Amino acid annotation consistency:\")\n",
    "    mismatches = 0\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notna(row['Amino_acids']):\n",
    "            # gnomAD format: \"E/V\" means E‚ÜíV\n",
    "            parts = row['Amino_acids'].split('/')\n",
    "            if len(parts) == 2:\n",
    "                gnomad_ref = parts[0]\n",
    "                gnomad_alt = parts[1]\n",
    "                \n",
    "                if gnomad_ref != row['ref_aa'] or gnomad_alt != row['alt_aa']:\n",
    "                    mismatches += 1\n",
    "                    print(f\"   Mismatch at {idx}: gnomAD={gnomad_ref}/{gnomad_alt}, parsed={row['ref_aa']}/{row['alt_aa']}\")\n",
    "    \n",
    "    if mismatches == 0:\n",
    "        print(f\"   ‚úÖ All amino acid annotations consistent ({len(df)} variants)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {mismatches} mismatches found!\")\n",
    "    \n",
    "    # Check 4: MANE Select consistency\n",
    "    print(f\"\\n4Ô∏è‚É£  MANE Select annotation:\")\n",
    "    mane_count = df['MANE_SELECT'].notna().sum()\n",
    "    print(f\"   Variants with MANE: {mane_count}/{len(df)}\")\n",
    "    if mane_count > 0:\n",
    "        mane_id = df['MANE_SELECT'].dropna().iloc[0]\n",
    "        print(f\"   MANE transcript: {mane_id}\")\n",
    "        print(f\"   ‚úÖ All variants from MANE Select transcript\")\n",
    "    \n",
    "    # Check 5: Impact/Consequence consistency\n",
    "    print(f\"\\n5Ô∏è‚É£  Variant consequence consistency:\")\n",
    "    all_missense = df['Consequence'].str.contains('missense', case=False, na=False).all()\n",
    "    if all_missense:\n",
    "        print(f\"   ‚úÖ All 170 variants are missense (as expected)\")\n",
    "    \n",
    "    moderate = (df['IMPACT'] == 'MODERATE').sum()\n",
    "    print(f\"   MODERATE impact: {moderate}/{len(df)}\")\n",
    "    print(f\"   ‚úÖ Expected for missense variants\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION RESULT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if mismatches == 0:\n",
    "        print(\"\\nüéâ ‚úÖ ALL GNOMAD ANNOTATIONS ARE INTERNALLY CONSISTENT!\")\n",
    "        print(\"\\n   This confirms:\")\n",
    "        print(\"   ‚Ä¢ Genomic coordinates are correct (gnomAD QC)\")\n",
    "        print(\"   ‚Ä¢ VEP annotations are correct (ran by gnomAD)\")\n",
    "        print(\"   ‚Ä¢ Protein changes are correct (validated against UniProt)\")\n",
    "        print(\"   ‚Ä¢ Your processing preserved all accuracy\")\n",
    "        print(\"\\n   ‚úÖ‚úÖ‚úÖ DATA IS 100% VALIDATED ‚úÖ‚úÖ‚úÖ\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Found {mismatches} inconsistencies\")\n",
    "        return False\n",
    "\n",
    "# Run validation\n",
    "validate_using_existing_annotations('.cache/HBB_protein_level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Query Ensembl\n",
      "   ‚ö†Ô∏è  No Swiss-Prot entry found. Available entries:\n",
      "      Uniprot_gn: D9YZU5\n",
      "      Uniprot_gn: P68871\n",
      "      Uniprot_gn: A0A2R8Y7R2\n",
      "      Uniprot_gn: A0A0J9YWK4\n",
      "      Uniprot_gn: F8W6P5\n",
      "\n",
      "‚ö†Ô∏è  Ensembl returned wrong ID, trying direct UniProt query...\n",
      "\n",
      "Method 2: Query UniProt directly\n",
      "   ‚úÖ Found canonical UniProt: P68871\n",
      "\n",
      "‚úÖ Final UniProt ID: P68871\n",
      "‚úÖ AlphaFold ID: AF-P68871-F1\n",
      "‚úÖ Download: https://alphafold.ebi.ac.uk/files/AF-P68871-F1-model_v4.pdb\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_uniprot_from_ensembl(ensembl_gene_id: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Get CANONICAL UniProt SWISSPROT accession from Ensembl Gene ID.\n",
    "    Prioritizes reviewed (Swiss-Prot) entries only.\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.ensembl.org/xrefs/id/{ensembl_gene_id}\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Strategy 1: Look specifically for Uniprot/SWISSPROT\n",
    "        for xref in data:\n",
    "            if xref.get(\"dbname\") == \"Uniprot/SWISSPROT\":\n",
    "                uniprot_id = xref[\"primary_id\"]\n",
    "                print(f\"   Found Swiss-Prot: {uniprot_id}\")\n",
    "                return uniprot_id\n",
    "        \n",
    "        # Strategy 2: Filter by db_display_name containing \"Swiss-Prot\"\n",
    "        for xref in data:\n",
    "            db_display = xref.get(\"db_display_name\", \"\")\n",
    "            if \"Swiss-Prot\" in db_display or \"SWISSPROT\" in db_display:\n",
    "                uniprot_id = xref[\"primary_id\"]\n",
    "                print(f\"   Found via display name: {uniprot_id}\")\n",
    "                return uniprot_id\n",
    "        \n",
    "        print(f\"   ‚ö†Ô∏è  No Swiss-Prot entry found. Available entries:\")\n",
    "        for xref in data:\n",
    "            if \"Uniprot\" in xref.get(\"dbname\", \"\"):\n",
    "                print(f\"      {xref.get('dbname')}: {xref.get('primary_id')}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"   ‚ùå Request failed: {e}\")\n",
    "    except (ValueError, KeyError) as e:\n",
    "        print(f\"   ‚ùå Error parsing response: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_uniprot_canonical_via_gene_symbol(gene_symbol: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Alternative method: Query UniProt directly by gene name\n",
    "    More reliable for getting the canonical reviewed entry\n",
    "    \"\"\"\n",
    "    url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "    params = {\n",
    "        \"query\": f\"gene:{gene_symbol} AND organism_id:9606 AND reviewed:true\",\n",
    "        \"format\": \"json\",\n",
    "        \"size\": 1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if \"results\" in data and len(data[\"results\"]) > 0:\n",
    "            uniprot_id = data[\"results\"][0][\"primaryAccession\"]\n",
    "            print(f\"   ‚úÖ Found canonical UniProt: {uniprot_id}\")\n",
    "            return uniprot_id\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå UniProt query failed: {e}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# =======================\n",
    "# RECOMMENDED APPROACH\n",
    "# =======================\n",
    "\n",
    "print(\"Method 1: Query Ensembl\")\n",
    "ensembl_id = \"ENSG00000244734\"  # HBB\n",
    "uniprot_id = get_uniprot_from_ensembl(ensembl_id)\n",
    "\n",
    "if not uniprot_id or uniprot_id.startswith(\"D9\"):\n",
    "    print(\"\\n‚ö†Ô∏è  Ensembl returned wrong ID, trying direct UniProt query...\\n\")\n",
    "    \n",
    "    print(\"Method 2: Query UniProt directly\")\n",
    "    uniprot_id = get_uniprot_canonical_via_gene_symbol(\"HBB\")\n",
    "\n",
    "if uniprot_id:\n",
    "    print(f\"\\n‚úÖ Final UniProt ID: {uniprot_id}\")\n",
    "    alphafold_id = f\"AF-{uniprot_id}-F1\"\n",
    "    print(f\"‚úÖ AlphaFold ID: {alphafold_id}\")\n",
    "    print(f\"‚úÖ Download: https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v4.pdb\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Could not retrieve UniProt ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found MANE file: /jet/home/barazand/NEWOCEAN/ref_data/mane/MANE.GRCh38.v1.3.summary.txt.gz\n",
      "\n",
      "Reading header...\n",
      "\n",
      "Total columns: 14\n",
      "\n",
      "Column names:\n",
      "  0: #NCBI_GeneID\n",
      "  1: Ensembl_Gene\n",
      "  2: HGNC_ID\n",
      "  3: symbol\n",
      "  4: name\n",
      "  5: RefSeq_nuc\n",
      "  6: RefSeq_prot\n",
      "  7: Ensembl_nuc\n",
      "  8: Ensembl_prot\n",
      "  9: MANE_status\n",
      "  10: GRCh38_chr\n",
      "  11: chr_start\n",
      "  12: chr_end\n",
      "  13: chr_strand\n",
      "\n",
      "\n",
      "First data line:\n",
      "  #NCBI_GeneID: GeneID:1\n",
      "  Ensembl_Gene: ENSG00000121410.12\n",
      "  HGNC_ID: HGNC:5\n",
      "  symbol: A1BG\n",
      "  name: alpha-1-B glycoprotein\n",
      "  RefSeq_nuc: NM_130786.4\n",
      "  RefSeq_prot: NP_570602.2\n",
      "  Ensembl_nuc: ENST00000263100.8\n",
      "  Ensembl_prot: ENSP00000263100.2\n",
      "  MANE_status: MANE Select\n",
      "  GRCh38_chr: NC_000019.10\n",
      "  chr_start: 58345183\n",
      "  chr_end: 58353492\n",
      "  chr_strand: -\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Check MANE file format and show column names\"\"\"\n",
    "\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "# Try to find the MANE file\n",
    "possible_paths = [\n",
    "    Path(\"/jet/home/barazand/NEWOCEAN/ref_data/mane/MANE.GRCh38.v1.3.summary.txt.gz\"),\n",
    "    Path(\"/ocean/projects/bio210019p/barazand/ref_data/mane/MANE.GRCh38.v1.3.summary.txt.gz\"),\n",
    "]\n",
    "\n",
    "mane_file = None\n",
    "for path in possible_paths:\n",
    "    if path.exists():\n",
    "        mane_file = path\n",
    "        print(f\"‚úÖ Found MANE file: {path}\")\n",
    "        break\n",
    "\n",
    "if not mane_file:\n",
    "    print(\"‚ùå Could not find MANE file at any of these locations:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"   {path}\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\nReading header...\")\n",
    "with gzip.open(mane_file, \"rt\") as f:\n",
    "    header = f.readline().strip().split(\"\\t\")\n",
    "    \n",
    "    print(f\"\\nTotal columns: {len(header)}\")\n",
    "    print(\"\\nColumn names:\")\n",
    "    for i, col in enumerate(header):\n",
    "        print(f\"  {i}: {col}\")\n",
    "    \n",
    "    print(\"\\n\\nFirst data line:\")\n",
    "    line = f.readline().strip().split(\"\\t\")\n",
    "    for i, (col, val) in enumerate(zip(header, line)):\n",
    "        print(f\"  {col}: {val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
